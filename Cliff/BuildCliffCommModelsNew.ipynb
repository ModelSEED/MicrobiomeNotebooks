{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notable Narratives and content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collective repository\n",
    "- GitHub:   https://github.com/ModelSEED/MicrobiomeNotebooks.git\n",
    "\n",
    "# utilized Narratives\n",
    "- MAG genomes: https://narrative.kbase.us/narrative/155805 & https://narrative.kbase.us/narrative/163264\n",
    "- Pangenome-enhanced genomes: https://narrative.kbase.us/narrative/187797\n",
    "- Annotated pangenome-enhanced genomes: https://narrative.kbase.us/narrative/188040\n",
    "- Reconstructed MAG models: https://narrative.kbase.us/narrative/186678\n",
    "\n",
    "# previously-computed files\n",
    "- FAA of representative sequences: /scratch/fliu/data/cliff/mmseqs_ani_prob_rep_genome.faa\n",
    "- Cluster genome mapping: /scratch/fliu/data/cliff/mmseqs_ani_prob.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genome_ws = [155805, 163264]\n",
    "mag_ws = 188065\n",
    "new_workspace = 187471\n",
    "pangenome_mag_ws = 187040\n",
    "prob_mapping = \"/scratch/fliu/data/cliff/mmseqs_ani_prob_v2.json\"\n",
    "prob_rep_mapping = \"/scratch/fliu/data/cliff/mmseqs_ani_prob_rep_genome_v2.faa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading hit data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "with open(\"/scratch/fliu/data/cliff/mmseqs_ani_prob_v2.json\", 'r') as f:\n",
    "    hit_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing hits for close genome list and establishing thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_ani_data = {}\n",
    "mag_thresholds = {}\n",
    "mag_genomes = {}\n",
    "mag_hit_distribution = {}\n",
    "max_bins = 27\n",
    "min_sim = 74\n",
    "for protein in hit_data:\n",
    "    for mag in hit_data[protein]:\n",
    "        if mag not in mag_ani_data:\n",
    "            mag_ani_data[mag] = {}\n",
    "            mag_hit_distribution[mag] = [0] * max_bins\n",
    "        for hit in hit_data[protein][mag]:\n",
    "            genome = hit.split(\":\")[0]\n",
    "            if hit_data[protein][mag][hit] < 100 and hit_data[protein][mag][hit] >= min_sim:\n",
    "                if genome not in mag_ani_data[mag]:\n",
    "                    mag_hit_distribution[mag][int(hit_data[protein][mag][hit]-min_sim)] += 1\n",
    "                mag_ani_data[mag][genome] = hit_data[protein][mag][hit]\n",
    "util.save(\"mag_ani_data\",mag_ani_data)\n",
    "util.save(\"mag_hit_distribution\",mag_hit_distribution)\n",
    "records = []\n",
    "for mag in mag_hit_distribution:\n",
    "    if mag not in mag_thresholds:\n",
    "        mag_thresholds[mag] = {\"threshold\":0,\"threshold_count\":0}\n",
    "    record = {\"mag\":mag}\n",
    "    threshold = 0\n",
    "    for i in range(max_bins):\n",
    "        record[\"ani_\" + str(min_sim+i)] = mag_hit_distribution[mag][i]\n",
    "    threshold_count = 0\n",
    "    for i in range(max_bins):\n",
    "        index = max_bins-1-i  \n",
    "        if mag_hit_distribution[mag][index] > 0 and (threshold == 0 or threshold == min_sim+index+1) and threshold_count <= 4 and (threshold_count == 0 or mag_hit_distribution[mag][index]/threshold_count < 5):\n",
    "            threshold_count += mag_hit_distribution[mag][index]\n",
    "            threshold = min_sim+index\n",
    "    mag_thresholds[mag][\"threshold\"] = threshold\n",
    "    mag_thresholds[mag][\"threshold_count\"] = threshold_count\n",
    "    if threshold_count == 1:\n",
    "        new_threshold = 0\n",
    "        additional_count = 0\n",
    "        start = max_bins-(threshold-min_sim)\n",
    "        for i in range(start,max_bins,1):\n",
    "            index = max_bins-1-i\n",
    "            if mag_hit_distribution[mag][index] > 0 and (new_threshold == 0 or new_threshold == min_sim+index+1) and additional_count <= 4 and (additional_count == 0 or mag_hit_distribution[mag][index]/additional_count < 5):\n",
    "                additional_count += mag_hit_distribution[mag][index]\n",
    "                new_threshold = min_sim+index\n",
    "        if new_threshold != 0:\n",
    "            record[\"new_threshold\"] = new_threshold\n",
    "            record[\"new_threshold_count\"] = additional_count\n",
    "            if (threshold-new_threshold)<=10 or threshold != 100:\n",
    "                mag_thresholds[mag][\"threshold\"] = new_threshold\n",
    "                mag_thresholds[mag][\"threshold_count\"] = threshold_count+additional_count\n",
    "    record[\"threshold\"] = threshold\n",
    "    record[\"threshold_count\"] = threshold_count\n",
    "    records.append(record)\n",
    "for mag in mag_ani_data:\n",
    "    mag_genomes[mag] = {}\n",
    "    for genome in mag_ani_data[mag]:\n",
    "        if mag_ani_data[mag][genome] >= mag_thresholds[mag][\"threshold\"]:\n",
    "            mag_genomes[mag][genome] = mag_ani_data[mag][genome]\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(util.output_dir+\"/mag_hit_distribution.csv\",index=False)\n",
    "util.save(\"mag_thresholds\",mag_thresholds)\n",
    "util.save(\"mag_genomes\",mag_genomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning thresholds manually (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "mag_ani_data = util.load(\"mag_ani_data\")\n",
    "mag_genomes = {}\n",
    "for mag in mag_ani_data:\n",
    "    mag_genomes[mag] = {}\n",
    "    for genome in mag_ani_data[mag]:\n",
    "        if mag_ani_data[mag][genome] >= mag_thresholds[mag][\"threshold\"]:\n",
    "            mag_genomes[mag][genome] = mag_ani_data[mag][genome]\n",
    "    mag_thresholds[mag][\"threshold_count\"] = len(mag_genomes[mag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting workspace IDs for MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_wsids = {}\n",
    "for ws in genome_ws:\n",
    "    for mag in util.msrecon.kbase_api.list_objects(ws, object_type=\"KBaseGenomes.Genome\", include_metadata=False):\n",
    "        mag_wsids[mag[1]] = mag[7]\n",
    "util.save(\"mag_wsids\",mag_wsids)\n",
    "util.save(\"mag_list\",mag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying supplemental proteins for MAGs based on thresholds and hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m         all_mag_deep_data[magname][protein][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     61\u001b[0m         all_mag_deep_data[magname][protein][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenomes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 62\u001b[0m     genomes_found[genome] \u001b[38;5;241m=\u001b[39m hit_data[protein][mag][hit]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m genomes_found[genome] \u001b[38;5;241m!=\u001b[39m hit_data[protein][mag][hit]:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: different similarity values for the same genome\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_protein_supplements = {}\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "mag_genomes = util.load(\"mag_genomes\")\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_wsids = util.load(\"mag_wsids\")\n",
    "magNames = util.load(\"mag_to_name\")\n",
    "all_mag_deep_data = {}\n",
    "for magname in mag_wsids:\n",
    "    all_mag_deep_data[magname] = {}\n",
    "for protein in hit_data:\n",
    "    for mag in hit_data[protein]:\n",
    "        magname = magNames[mag]+\".pg.G.D\"\n",
    "        if mag_thresholds[mag][\"threshold\"] > 0:\n",
    "            nr_mags_found = {}\n",
    "            rejected_mags_found = {}\n",
    "            genomes_found = {}\n",
    "            all_mag_deep_data[magname][protein] = {\n",
    "                \"self\":False,\n",
    "                \"total\":[0,0],\n",
    "                \"nr_mags\":[0,0],\n",
    "                \"rejected_mags\":[0,0],\n",
    "                \"genomes\":[0,0]\n",
    "            }\n",
    "            for hit in hit_data[protein][mag]:\n",
    "                (genome,gene) = hit.split(\":\")\n",
    "                if genome == \"self\":\n",
    "                    all_mag_deep_data[magname][protein][\"self\"] = True\n",
    "                elif genome in mag_list:\n",
    "                    if genome not in nr_mags_found:\n",
    "                        all_mag_deep_data[magname][protein][\"total\"][0] += 1\n",
    "                        all_mag_deep_data[magname][protein][\"nr_mags\"][0] += 1\n",
    "                        if hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]:\n",
    "                            if genome not in mag_genomes[mag]:\n",
    "                                print(genome+\":\"+mag+\":\"+str(hit_data[protein][mag][hit])+\"/\"+str(mag_thresholds[mag][\"threshold\"]))\n",
    "                            all_mag_deep_data[magname][protein][\"total\"][1] += 1\n",
    "                            all_mag_deep_data[magname][protein][\"nr_mags\"][1] += 1\n",
    "                        nr_mags_found[genome] = hit_data[protein][mag][hit]\n",
    "                    elif nr_mags_found[genome] != hit_data[protein][mag][hit]:\n",
    "                        print(\"Warning: different similarity values for the same genome\")\n",
    "                elif genome in mag_wsids:\n",
    "                    if genome not in rejected_mags_found:\n",
    "                        all_mag_deep_data[magname][protein][\"total\"][0] += 1\n",
    "                        all_mag_deep_data[magname][protein][\"rejected_mags\"][0] += 1\n",
    "                        if hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]:\n",
    "                            if genome not in mag_genomes[mag]:\n",
    "                                print(genome+\":\"+mag+\":\"+str(hit_data[protein][mag][hit])+\"/\"+str(mag_thresholds[mag][\"threshold\"]))\n",
    "                            all_mag_deep_data[magname][protein][\"total\"][1] += 1\n",
    "                            all_mag_deep_data[magname][protein][\"rejected_mags\"][1] += 1\n",
    "                        rejected_mags_found[genome] = hit_data[protein][mag][hit]\n",
    "                    elif rejected_mags_found[genome] != hit_data[protein][mag][hit]:\n",
    "                        print(\"Warning: different similarity values for the same genome\")\n",
    "                else:\n",
    "                    if genome not in genomes_found:\n",
    "                        all_mag_deep_data[magname][protein][\"total\"][0] += 1\n",
    "                        all_mag_deep_data[magname][protein][\"genomes\"][0] += 1\n",
    "                        if hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]:\n",
    "                            if genome not in mag_genomes[mag]:\n",
    "                                print(genome+\":\"+mag+\":\"+str(hit_data[protein][mag][hit])+\"/\"+str(mag_thresholds[mag][\"threshold\"]))\n",
    "                            all_mag_deep_data[magname][protein][\"total\"][1] += 1\n",
    "                            all_mag_deep_data[magname][protein][\"genomes\"][1] += 1\n",
    "                        genomes_found[genome] = hit_data[protein][mag][hit]\n",
    "                    elif genomes_found[genome] != hit_data[protein][mag][hit]:\n",
    "                        print(\"Warning: different similarity values for the same genome\")\n",
    "            #We only add supplemental proteins for a family that does not already have a protein in the MAG\n",
    "            if not all_mag_deep_data[magname][protein][\"self\"]:\n",
    "                if mag not in mag_protein_supplements:\n",
    "                    mag_protein_supplements[mag] = {}\n",
    "                mag_protein_supplements[mag][protein] = [all_mag_deep_data[magname][protein][\"total\"][1],all_mag_deep_data[magname][protein][\"nr_mags\"][1],all_mag_deep_data[magname][protein][\"rejected_mags\"][1]]\n",
    "for magname in all_mag_deep_data:\n",
    "    util.save(\"deepdata/\"+mag,all_mag_deep_data[magname])\n",
    "util.save(\"mag_protein_supplements\",mag_protein_supplements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load functional data (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/chenry/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/kb_sdk_home/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "function_hit_data = json.load(open('/home/fliu/cliff_mags/data/annotation_ani_prob_lo_85.json'))\n",
    "mag_function_supplements = {}\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_wsids = util.load(\"mag_wsids\")\n",
    "all_func_mag_deep_data = {}\n",
    "for mag in mag_wsids:\n",
    "    all_func_mag_deep_data[mag] = {}\n",
    "for protein in function_hit_data:\n",
    "    for mag in function_hit_data[protein]:\n",
    "        nr_mags_found = {}\n",
    "        rejected_mags_found = {}\n",
    "        genomes_found = {}\n",
    "        all_func_mag_deep_data[mag][protein] = {\n",
    "            \"self\":False,\n",
    "            \"total\":[0,0],\n",
    "            \"nr_mags\":[0,0],\n",
    "            \"rejected_mags\":[0,0],\n",
    "            \"genomes\":[0,0]\n",
    "        }\n",
    "        for hit in function_hit_data[protein][mag]:\n",
    "            (genome,gene) = hit.split(\":\")\n",
    "            if genome == \"self\":\n",
    "                all_func_mag_deep_data[mag][protein][\"self\"] = True\n",
    "            elif genome in mag_list:\n",
    "                if genome not in nr_mags_found:\n",
    "                    all_func_mag_deep_data[mag][protein][\"total\"][0] += 1\n",
    "                    all_func_mag_deep_data[mag][protein][\"nr_mags\"][0] += 1\n",
    "                    if function_hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]/100:\n",
    "                        all_func_mag_deep_data[mag][protein][\"total\"][1] += 1\n",
    "                        all_func_mag_deep_data[mag][protein][\"nr_mags\"][1] += 1\n",
    "                    nr_mags_found[genome] = function_hit_data[protein][mag][hit]\n",
    "                elif nr_mags_found[genome] != function_hit_data[protein][mag][hit]:\n",
    "                    print(\"Warning: different similarity values for the same genome\")\n",
    "            elif genome in mag_wsids:\n",
    "                if genome not in rejected_mags_found:\n",
    "                    all_func_mag_deep_data[mag][protein][\"total\"][0] += 1\n",
    "                    all_func_mag_deep_data[mag][protein][\"rejected_mags\"][0] += 1\n",
    "                    if function_hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]/100:\n",
    "                        all_func_mag_deep_data[mag][protein][\"total\"][1] += 1\n",
    "                        all_func_mag_deep_data[mag][protein][\"rejected_mags\"][1] += 1\n",
    "                    rejected_mags_found[genome] = function_hit_data[protein][mag][hit]\n",
    "                elif rejected_mags_found[genome] != function_hit_data[protein][mag][hit]:\n",
    "                    print(\"Warning: different similarity values for the same genome\")\n",
    "            else:\n",
    "                if genome not in genomes_found:\n",
    "                    all_func_mag_deep_data[mag][protein][\"total\"][0] += 1\n",
    "                    all_func_mag_deep_data[mag][protein][\"genomes\"][0] += 1\n",
    "                    if function_hit_data[protein][mag][hit] >= mag_thresholds[mag][\"threshold\"]/100:\n",
    "                        all_func_mag_deep_data[mag][protein][\"total\"][1] += 1\n",
    "                        all_func_mag_deep_data[mag][protein][\"genomes\"][1] += 1\n",
    "                    genomes_found[genome] = function_hit_data[protein][mag][hit]\n",
    "                elif genomes_found[genome] != function_hit_data[protein][mag][hit]:\n",
    "                    print(\"Warning: different similarity values for the same genome\")\n",
    "        #We only add supplemental proteins for a family that does not already have a protein in the MAG\n",
    "        if not all_func_mag_deep_data[mag][protein][\"self\"]:\n",
    "            if mag not in mag_function_supplements:\n",
    "                mag_function_supplements[mag] = {}\n",
    "            mag_function_supplements[mag][protein] = [all_func_mag_deep_data[mag][protein][\"total\"][1],all_func_mag_deep_data[mag][protein][\"nr_mags\"][1],all_func_mag_deep_data[mag][protein][\"rejected_mags\"][1]]\n",
    "for mag in mag_function_supplements:\n",
    "    util.save(\"funcdeepdata/\"+mag,all_func_mag_deep_data[mag])\n",
    "util.save(\"mag_function_supplements\",mag_function_supplements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing stats on additional proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.8.contigs__.RAST 0.1 14737\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.20.contigs__.RAST 0.1 3860\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.4.contigs__.RAST 0.1 4004\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 0.1 4192\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_metabat.31.contigs__.RAST 0.1 5106\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_concoct_out.29.contigs__.RAST 0.1 3670\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.50.contigs__.RAST 0.1 4271\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.18.contigs__.RAST 0.1 4144\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.15.contigs__.RAST 0.1 1457\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.88.contigs__.RAST 0.1 2330\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.57.contigs__.RAST 0.1 1738\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.10.contigs__.RAST 0.1 2713\n",
      "Salt_Pond_MetaGSF2_B_D1_MG_DASTool_bins_concoct_out.44.contigs__.RAST 0.1 3227\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.51.contigs__.RAST 0.1 1178\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_concoct_out.69.contigs__.RAST 0.1 1707\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 552\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.18.contigs__.RAST 0.1 1601\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_metabat.10.contigs__.RAST 0.1 526\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 0.1 4054\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_concoct_out.67.contigs__.RAST 0.1 4079\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.25.contigs__.RAST 0.1 1642\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.50.contigs__.RAST 0.1 3743\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.29.contigs__.RAST 0.1 2511\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.38.contigs__.RAST 0.1 7764\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.80.contigs__.RAST 0.1 1130\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.13.contigs__.RAST 0.1 1058\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.45.contigs__.RAST 0.1 4868\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.17.contigs__.RAST 0.15 12879\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.4.contigs__.RAST 0.1 11687\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.28.contigs__.RAST 0.1 2667\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_concoct_out.62.contigs__.RAST 0.1 3462\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.42.contigs__.RAST 0.1 3490\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.14.contigs__.RAST 0.1 12246\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.17.contigs__.RAST 0.1 2856\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_concoct_out.91.contigs__.RAST 0.1 1994\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_metabat.22.contigs__.RAST 0.1 12202\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.34.contigs__.RAST 0.1 6051\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.19.contigs__.RAST 0.1 17763\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.37.contigs__.RAST 0.1 13720\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.1.contigs__.RAST 0.1 770\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 7787\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.52.contigs__.RAST 0.1 4103\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.120.contigs__.RAST 0.1 4530\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_metabat.27.contigs__.RAST 0.1 19814\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_concoct_out.45.contigs__.RAST 0.1 4509\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.9.contigs__.RAST 0.1 10271\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.46.contigs__.RAST 0.1 8976\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_metabat.6.contigs__.RAST 0.1 9104\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_concoct_out.67.contigs__.RAST 0.1 2987\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.45.contigs__.RAST 0.1 6886\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.15.contigs__.RAST 0.1 3915\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.69.contigs__.RAST 0.1 4127\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_metabat.14.contigs__.RAST 0.1 7188\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.23.contigs__.RAST 0.1 13334\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.48.contigs__.RAST 0.1 3590\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.73.contigs__.RAST 0.1 9701\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.143.contigs__.RAST 0.1 4194\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.78.contigs__.RAST 0.1 765\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_concoct_out.67.contigs__.RAST 0.1 7983\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.12.contigs__.RAST 0.1 3828\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.18.contigs__.RAST 0.1 4579\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.40.contigs__.RAST 0.1 5366\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.87.contigs__.RAST 0.1 4720\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 2249\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.19.contigs__.RAST 0.1 1531\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_maxbin.009.contigs__.RAST 0.1 4035\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_concoct_out.33.contigs__.RAST 0.1 1541\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_metabat.10.contigs__.RAST 0.1 8126\n",
      "Salt_Pond_MetaG_R2_B_H2O_MG_DASTool_bins_metabat.17.contigs__.RAST 0.1 4179\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.51.contigs__.RAST 0.1 3660\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.30.contigs__.RAST 0.1 6809\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.42.contigs__.RAST 0.1 5624\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.26.contigs__.RAST 0.1 1764\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.68.contigs__.RAST 0.1 17088\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_metabat.19.contigs__.RAST 0.1 1389\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.44.contigs__.RAST 0.1 5876\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.63.contigs__.RAST 0.1 1080\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.7.contigs__.RAST 0.1 10147\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_concoct_out.59.contigs__.RAST 0.1 8104\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.2.contigs__.RAST 0.1 6308\n",
      "Salt_Pond_MetaGSF2_B_D2_MG_DASTool_bins_concoct_out.42.contigs__.RAST 0.1 11945\n",
      "Salt_Pond_MetaG_R2A_B_D1_MG_DASTool_bins_concoct_out.4.contigs__.RAST 0.1 11102\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.9.contigs__.RAST 0.1 7971\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.8.contigs__.RAST 0.1 9888\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.6.contigs__.RAST 0.1 1836\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.13.contigs__.RAST 0.1 8654\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.19.contigs__.RAST 0.1 4445\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_concoct_out.19.contigs__.RAST 0.1 4457\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 0.1 4795\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.7.contigs__.RAST 0.1 7154\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_metabat.29.contigs__.RAST 0.1 4688\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_concoct_out.34.contigs__.RAST 0.1 1332\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.32.contigs__.RAST 0.1 4894\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_concoct_out.45.contigs__.RAST 0.1 10269\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.100.contigs__.RAST 0.1 2770\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 1746\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_metabat.42.contigs__.RAST 0.1 647\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.28.contigs__.RAST 0.1 8488\n",
      "Salt_Pond_MetaGSF2_C_D1_MG_DASTool_bins_concoct_out.13.contigs__.RAST 0.1 2035\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.15.contigs__.RAST 0.1 10326\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.11.contigs__.RAST 0.1 10389\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.26.contigs__.RAST 0.1 5937\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_concoct_out.89.contigs__.RAST 0.1 4252\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.28.contigs__.RAST 0.1 4318\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_metabat.13.contigs__.RAST 0.1 4140\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.25.contigs__.RAST 0.1 3750\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.39.contigs__.RAST 0.1 2514\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.89.contigs__.RAST 0.1 10168\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.45.contigs__.RAST 0.1 3140\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_maxbin.001.contigs__.RAST 0.1 2215\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.37.contigs__.RAST 0.1 1956\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_metabat.23.contigs__.RAST 0.1 1715\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.95.contigs__.RAST 0.15 7235\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_concoct_out.80.contigs__.RAST 0.1 2464\n",
      "Salt_Pond_MetaGSF2_B_D1_MG_DASTool_bins_metabat.9.contigs__.RAST 0.1 9423\n",
      "Salt_Pond_MetaGSF2_A_D1_MG_DASTool_bins_concoct_out.23.contigs__.RAST 0.1 4575\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.concoct_out.44.contigs__.RAST 0.1 2614\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_metabat.49.contigs__.RAST 0.1 491\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.4.contigs__.RAST 0.1 3572\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.38.contigs__.RAST 0.1 4211\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.36.contigs__.RAST 0.1 3144\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.58.contigs__.RAST 0.1 840\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_metabat.11.contigs__.RAST 0.1 5495\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_concoct_out.11.contigs__.RAST 0.1 4269\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_metabat.8.contigs__.RAST 0.1 10597\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_maxbin.002.contigs__.RAST 0.1 1469\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.119.contigs__.RAST 0.1 1908\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.39.contigs__.RAST 0.1 7301\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_metabat.17.contigs__.RAST 0.1 1611\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.47.contigs__.RAST 0.1 4654\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.21.contigs__.RAST 0.1 11100\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.5.contigs__.RAST 0.1 6377\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_concoct_out.78.contigs__.RAST 0.1 8338\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.39.contigs__.RAST 0.1 1096\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_maxbin.029.contigs__.RAST 0.1 3647\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.23.contigs__.RAST 0.1 887\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.45.contigs__.RAST 0.1 7845\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_metabat.5.contigs__.RAST 0.1 556\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.29.contigs__.RAST 0.1 1097\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.27.contigs__.RAST 0.1 2113\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_metabat.40.contigs__.RAST 0.1 3981\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_concoct_out.53.contigs__.RAST 0.1 2440\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_metabat.14.contigs__.RAST 0.1 4191\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.65.contigs__.RAST 0.1 9238\n",
      "Salt_Pond_MetaG_R2_B_H2O_MG_DASTool_bins_metabat.29.contigs__.RAST 0.1 591\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.11.contigs__.RAST 0.1 2988\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.18.contigs__.RAST 0.1 3452\n",
      "Salt_Pond_MetaG_R2_C_H2O_MG_DASTool_bins_metabat.29.contigs__.RAST 0.1 352\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_concoct_out.16.contigs__.RAST 0.1 571\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_maxbin.062.contigs__.RAST 0.1 6108\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.10.contigs__.RAST 0.1 10540\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.11.contigs__.RAST 0.1 2899\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.concoct_out.54.contigs__.RAST 0.1 7022\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.50.contigs__.RAST 0.1 3801\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.25.contigs__.RAST 0.1 3131\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.40.contigs__.RAST 0.1 1793\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_metabat.8.contigs__.RAST 0.1 4899\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.49.contigs__.RAST 0.1 10297\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.20.contigs__.RAST 0.1 752\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_metabat.5.contigs__.RAST 0.1 4011\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_metabat.25.contigs__.RAST 0.1 4998\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_maxbin.005.contigs__.RAST 0.1 3128\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.98.contigs__.RAST 0.1 675\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.45.contigs__.RAST 0.1 9497\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_metabat.22.contigs__.RAST 0.1 8159\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_maxbin.047.contigs__.RAST 0.1 3398\n",
      "Salt_Pond_MetaG_R2_C_H2O_MG_DASTool_bins_concoct_out.60.contigs__.RAST 0.1 3690\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_metabat.22.contigs__.RAST 0.1 1781\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_maxbin.022.contigs__.RAST 0.1 5485\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.24.contigs__.RAST 0.1 1043\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.37.contigs__.RAST 0.1 10735\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 1230\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.18.contigs__.RAST 0.1 11145\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.109.contigs__.RAST 0.1 9491\n",
      "Salt_Pond_MetaG_R2_B_H2O_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 4400\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.19.contigs__.RAST 0.1 10178\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_metabat.30.contigs__.RAST 0.1 1342\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_concoct_out.40.contigs__.RAST 0.1 7966\n",
      "Salt_Pond_MetaG_R2_C_H2O_MG_DASTool_bins_metabat.20.contigs__.RAST 0.1 4850\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_concoct_out.59.contigs__.RAST 0.1 9419\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.43.contigs__.RAST 0.1 11786\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_metabat.58.contigs__.RAST 0.1 250\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_metabat.17.contigs__.RAST 0.1 12216\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.concoct_out.9.contigs__.RAST 0.1 5455\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.33.contigs__.RAST 0.1 4450\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.34.contigs__.RAST 0.1 1077\n",
      "Salt_Pond_MetaGSF2_B_D2_MG_DASTool_bins_concoct_out.55.contigs__.RAST 0.1 1024\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_concoct_out.11.contigs__.RAST 0.1 1395\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.28.contigs__.RAST 0.1 9227\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_metabat.6.contigs__.RAST 0.1 1645\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 4132\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_maxbin.028.contigs__.RAST 0.1 3787\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.49.contigs__.RAST 0.1 4372\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_concoct_out.9.contigs__.RAST 0.1 6576\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_metabat.7.contigs__.RAST 0.15 8394\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_metabat.13.contigs__.RAST 0.1 2270\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_concoct_out.44.contigs__.RAST 0.1 1294\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.84.contigs__.RAST 0.1 5422\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.31.contigs__.RAST 0.1 3853\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.37.contigs__.RAST 0.1 6045\n",
      "Salt_Pond_MetaG_R2_C_H2O_MG_DASTool_bins_concoct_out.35.contigs__.RAST 0.1 9582\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_metabat.20.contigs__.RAST 0.1 1421\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.34.contigs__.RAST 0.1 1723\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 0.1 911\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.3.contigs__.RAST 0.1 2642\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 0.1 2766\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.8.contigs__.RAST 0.15 10535\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_metabat.4.contigs__.RAST 0.1 1760\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_concoct_out.14.contigs__.RAST 0.1 1393\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.11.contigs__.RAST 0.1 8787\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 0.1 1970\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.32.contigs__.RAST 0.1 5166\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.17.contigs__.RAST 0.1 1544\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_concoct_out.85.contigs__.RAST 0.1 1296\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.55.contigs__.RAST 0.1 4325\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_maxbin.035.contigs__.RAST 0.1 311\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.14.contigs__.RAST 0.1 7677\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.29.contigs__.RAST 0.1 4008\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.17.contigs__.RAST 0.1 9913\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_concoct_out.79.contigs__.RAST 0.1 5133\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_concoct_out.54.contigs__.RAST 0.1 2713\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 0.1 12251\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.42.contigs__.RAST 0.1 3277\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.concoct_out.5.contigs__.RAST 0.1 3773\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_concoct_out.23.contigs__.RAST 0.1 3415\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.13.contigs__.RAST 0.1 2634\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.41.contigs__.RAST 0.1 12087\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 9561\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_concoct_out.2.contigs__.RAST 0.1 1451\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.49.contigs__.RAST 0.1 4030\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.27.contigs__.RAST 0.1 1436\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.52.contigs__.RAST 0.1 436\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.124.contigs__.RAST 0.1 2107\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.22.contigs__.RAST 0.1 10477\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.41.contigs__.RAST 0.45 19372\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 0.1 638\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.38.contigs__.RAST 0.1 5384\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.66.contigs__.RAST 0.1 953\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.52.contigs__.RAST 0.1 1832\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.27.contigs__.RAST 0.1 13032\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 0.35 17490\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.38.contigs__.RAST 0.1 6478\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.28.contigs__.RAST 0.1 1049\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 0.1 537\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_metabat.14.contigs__.RAST 0.1 1923\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_maxbin.014.contigs__.RAST 0.1 2998\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_concoct_out.36.contigs__.RAST 0.1 2672\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_concoct_out.110.contigs__.RAST 0.1 3013\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.47.contigs__.RAST 0.1 543\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.50.contigs__.RAST 0.1 9147\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_metabat.17.contigs__.RAST 0.1 3891\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.10.contigs__.RAST 0.1 10559\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.16.contigs__.RAST 0.1 11613\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.24.contigs__.RAST 0.1 10255\n",
      "Salt_Pond_MetaGSF2_B_D2_MG_DASTool_bins_metabat.4.contigs__.RAST 0.1 3705\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.55.contigs__.RAST 0.1 5072\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.84.contigs__.RAST 0.1 2519\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 0.1 3782\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.13.contigs__.RAST 0.1 3338\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.18.contigs__.RAST 0.1 869\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.10.contigs__.RAST 0.1 3442\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.35.contigs__.RAST 0.1 9349\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.40.contigs__.RAST 0.1 4159\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.19.contigs__.RAST 0.1 637\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.32.contigs__.RAST 0.1 3215\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_maxbin.017.contigs__.RAST 0.1 4145\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.93.contigs__.RAST 0.1 529\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.15.contigs__.RAST 0.1 1175\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.10.contigs__.RAST 0.1 10805\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 0.1 2509\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.8.contigs__.RAST 0.1 4095\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "#mag_protein_supplements = util.load(\"mag_protein_supplements\")\n",
    "records = []\n",
    "mag_probability_threshold = {}\n",
    "for mag in mag_list:\n",
    "    if mag[1] in mag_protein_supplements:\n",
    "        count = mag_thresholds[mag[1]][\"threshold_count\"]\n",
    "        record = {\"mag\":mag[1]}\n",
    "        records.append(record)\n",
    "        abundance_count = [0] * 400\n",
    "        for protein in mag_protein_supplements[mag[1]]:\n",
    "            fraction = mag_protein_supplements[mag[1]][protein][0]/count\n",
    "            entry = int(fraction*100/5)\n",
    "            abundance_count[entry] += 1\n",
    "        total = 0\n",
    "        final_total = None\n",
    "        mag_probability_threshold[mag[1]] = 0.1\n",
    "        for (i,entry) in enumerate(abundance_count):\n",
    "            index = len(abundance_count)-i-1\n",
    "            lasttotal = total\n",
    "            total += abundance_count[index]\n",
    "            if total >= 20000 and final_total == None:\n",
    "                threshold = 5*(index+1)/100\n",
    "                if threshold >= 0.1:\n",
    "                    mag_probability_threshold[mag[1]] = threshold\n",
    "                    final_total = lasttotal\n",
    "            if index == 2 and final_total == None:\n",
    "                final_total = total\n",
    "        print(mag[1],mag_probability_threshold[mag[1]],final_total)\n",
    "        for (i,entry) in enumerate(abundance_count):\n",
    "            record[i] = entry\n",
    "util.save(\"mag_probability_threshold\",mag_probability_threshold)\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(util.output_dir+\"/protein_count_abundance.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1722057900.422139 ERROR: Requested data mag_list doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/mag_list.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelSEED: /Users/chenry/code//kb_sdk/run_local/workdir/tmp/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested data mag_list doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/mag_list.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mag_to_name \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m name_to_mag \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m mag_list \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmag_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mag \u001b[38;5;129;01min\u001b[39;00m mag_list:\n\u001b[1;32m      6\u001b[0m     original_name \u001b[38;5;241m=\u001b[39m mag[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/code//chenry_utility_module/lib/chenry_utility_module/kbdevutils.py:147\u001b[0m, in \u001b[0;36mKBDevUtils.load\u001b[0;34m(self, name, default)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfilename)\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfilename))\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: Requested data mag_list doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/mag_list.json"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_to_name = {}\n",
    "name_to_mag = {}\n",
    "mag_list = util.load(\"mag_list\")\n",
    "for mag in mag_list:\n",
    "    original_name = mag[1]\n",
    "    new_name = original_name\n",
    "    new_name = new_name.replace(\"Salt_Pond_MetaG_\", \"\", 1)\n",
    "    new_name = new_name.replace(\"Salt_Pond_MetaG\", \"\", 1)\n",
    "    new_name = new_name.replace(\"_MG_DASTool_bins_\", \"\", 1)\n",
    "    new_name = new_name.replace(\"metabat\", \"\", 1)\n",
    "    new_name = new_name.replace(\"concoct_out\", \"\", 1)\n",
    "    new_name = new_name.replace(\"maxbin\", \"\", 1)\n",
    "    new_name = new_name.replace(\".contigs__.RAST\", \"\", 1)\n",
    "    mag_to_name[mag[1]] = new_name\n",
    "    name_to_mag[new_name] = mag[1]\n",
    "    print(name_to_mag)\n",
    "util.save(\"mag_to_name\",mag_to_name)\n",
    "util.save(\"name_to_mag\",name_to_mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading protein sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/chenry/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/kb_sdk_home/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "from Bio import SeqIO\n",
    "protein_hash = {}\n",
    "for record in SeqIO.parse('/scratch/fliu/data/cliff/mmseqs_ani_prob_rep_genome_v2.faa', 'fasta'):\n",
    "    protein_hash[record.id] = record.seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building genomes and assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/chenry/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/kb_sdk_home/run_local/workdir/tmp/\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.8.contigs__.RAST 14738\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.20.contigs__.RAST 3861\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.4.contigs__.RAST 4005\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.46.contigs__.RAST 4005\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 4193\n",
      "Salt_Pond_MetaG_R1_C_D1_MG_DASTool_bins_metabat.31.contigs__.RAST 5107\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_concoct_out.29.contigs__.RAST 3671\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.50.contigs__.RAST 4272\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.18.contigs__.RAST 4145\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.32.contigs__.RAST 4145\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.15.contigs__.RAST 1458\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.88.contigs__.RAST 2331\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.57.contigs__.RAST 1739\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.10.contigs__.RAST 2714\n",
      "Salt_Pond_MetaGSF2_B_D1_MG_DASTool_bins_concoct_out.44.contigs__.RAST 3228\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.51.contigs__.RAST 1179\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_concoct_out.69.contigs__.RAST 1708\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_metabat.16.contigs__.RAST 553\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.18.contigs__.RAST 1602\n",
      "Salt_Pond_MetaG_R2A_B_H2O_MG_DASTool_bins_metabat.10.contigs__.RAST 527\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 4055\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.86.contigs__.RAST 4055\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_concoct_out.67.contigs__.RAST 4080\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.25.contigs__.RAST 1643\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.50.contigs__.RAST 3744\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_metabat.2.contigs__.RAST 3744\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.29.contigs__.RAST 2512\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.38.contigs__.RAST 7765\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.80.contigs__.RAST 1131\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.13.contigs__.RAST 1059\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.45.contigs__.RAST 4869\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.17.contigs__.RAST 12880\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.4.contigs__.RAST 11688\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_metabat.28.contigs__.RAST 2668\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_concoct_out.62.contigs__.RAST 3463\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.42.contigs__.RAST 3491\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.14.contigs__.RAST 12247\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.17.contigs__.RAST 2857\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_concoct_out.91.contigs__.RAST 1995\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_metabat.22.contigs__.RAST 12203\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.34.contigs__.RAST 6052\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.19.contigs__.RAST 17764\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.37.contigs__.RAST 13721\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.1.contigs__.RAST 771\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.16.contigs__.RAST 7788\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.52.contigs__.RAST 4104\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.120.contigs__.RAST 4531\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.34.contigs__.RAST 1078\n",
      "Salt_Pond_MetaGSF2_B_D2_MG_DASTool_bins_concoct_out.55.contigs__.RAST 1025\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_concoct_out.11.contigs__.RAST 1396\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.28.contigs__.RAST 9228\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_metabat.6.contigs__.RAST 1646\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 4133\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_maxbin.028.contigs__.RAST 3788\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.49.contigs__.RAST 4373\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_concoct_out.9.contigs__.RAST 6577\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_metabat.7.contigs__.RAST 8395\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_metabat.13.contigs__.RAST 2271\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_concoct_out.44.contigs__.RAST 1295\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.84.contigs__.RAST 5423\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.31.contigs__.RAST 3854\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_concoct_out.37.contigs__.RAST 6046\n",
      "Salt_Pond_MetaG_R2_C_H2O_MG_DASTool_bins_concoct_out.35.contigs__.RAST 9583\n",
      "Salt_Pond_MetaG_R2A_A_H2O_MG_DASTool_bins_metabat.20.contigs__.RAST 1422\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_metabat.34.contigs__.RAST 1724\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 912\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.3.contigs__.RAST 2643\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 2767\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.8.contigs__.RAST 10536\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_metabat.16.contigs__.RAST 10536\n",
      "Salt_Pond_MetaG_R2A_C_H2O_MG_DASTool_bins_metabat.4.contigs__.RAST 1761\n",
      "Salt_Pond_MetaGSF2_B_H2O_MG_DASTool_bins_concoct_out.14.contigs__.RAST 1394\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.11.contigs__.RAST 8788\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 1971\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.32.contigs__.RAST 5167\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.17.contigs__.RAST 1545\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_concoct_out.85.contigs__.RAST 1297\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.20.contigs__.RAST 1297\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.55.contigs__.RAST 4326\n",
      "Salt_Pond_MetaG_R2_A_H2O_MG_DASTool_bins_maxbin.035.contigs__.RAST 312\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.14.contigs__.RAST 7678\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.37.contigs__.RAST 7678\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.29.contigs__.RAST 4009\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.17.contigs__.RAST 9914\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.8.contigs__.RAST 9914\n",
      "Salt_Pond_MetaG_R1_C_H2O_MG_DASTool_bins_concoct_out.79.contigs__.RAST 5134\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_concoct_out.54.contigs__.RAST 2714\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 12252\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.42.contigs__.RAST 3278\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.concoct_out.5.contigs__.RAST 3774\n",
      "Salt_Pond_MetaGSF2_A_H2O_MG_DASTool_bins_concoct_out.23.contigs__.RAST 3416\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.13.contigs__.RAST 2635\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.41.contigs__.RAST 12088\n",
      "Salt_Pond_MetaG_R2_A_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 9562\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_concoct_out.2.contigs__.RAST 1452\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.49.contigs__.RAST 4031\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.27.contigs__.RAST 1437\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.52.contigs__.RAST 437\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.24.contigs__.RAST 437\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_concoct_out.124.contigs__.RAST 2108\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.22.contigs__.RAST 10478\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.41.contigs__.RAST 19373\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.21.contigs__.RAST 639\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.38.contigs__.RAST 5385\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.6.contigs__.RAST 5385\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.66.contigs__.RAST 954\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.52.contigs__.RAST 1833\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.27.contigs__.RAST 13033\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_metabat.7.contigs__.RAST 13033\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 17491\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.38.contigs__.RAST 6479\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.28.contigs__.RAST 1050\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST 538\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.62.contigs__.RAST 538\n",
      "Salt_Pond_MetaG_R1_A_H2O_MG_DASTool_bins_metabat.14.contigs__.RAST 1924\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_maxbin.014.contigs__.RAST 2999\n",
      "Salt_Pond_MetaG_R1_C_D2_MG_DASTool_bins_concoct_out.36.contigs__.RAST 2673\n",
      "Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_concoct_out.110.contigs__.RAST 3014\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_concoct_out.7.contigs__.RAST 3014\n",
      "Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.12.contigs__.RAST 3014\n",
      "Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.47.contigs__.RAST 544\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_maxbin.037.contigs__.RAST 544\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.50.contigs__.RAST 9148\n",
      "Salt_Pond_MetaG_R2_C_D1_MG_DASTool_bins_metabat.17.contigs__.RAST 3892\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.10.contigs__.RAST 10560\n",
      "Salt_Pond_MetaG_R2A_B_D2_MG_DASTool_bins_concoct_out.47.contigs__.RAST 10560\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.16.contigs__.RAST 11614\n",
      "Salt_Pond_MetaG_R2A_A_D1_MG_DASTool_bins_concoct_out.24.contigs__.RAST 10256\n",
      "Salt_Pond_MetaGSF2_B_D2_MG_DASTool_bins_metabat.4.contigs__.RAST 3706\n",
      "Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.55.contigs__.RAST 5073\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_concoct_out.36.contigs__.RAST 5073\n",
      "Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_metabat.84.contigs__.RAST 2520\n",
      "Salt_Pond_MetaG_R1_B_D2_MG_DASTool_bins_metabat.16.contigs__.RAST 3783\n",
      "Salt_Pond_MetaGSF2_C_H2O_MG_DASTool_bins_metabat.13.contigs__.RAST 3339\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_metabat.18.contigs__.RAST 870\n",
      "Salt_Pond_MetaG_R2_B_D2_MG_DASTool_bins_metabat.10.contigs__.RAST 3443\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.35.contigs__.RAST 9350\n",
      "Salt_Pond_MetaG_R1_B_D1_MG_DASTool_bins_metabat.40.contigs__.RAST 4160\n",
      "Salt_Pond_MetaG_R1_B_H2O_MG_DASTool_bins_concoct_out.19.contigs__.RAST 638\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.32.contigs__.RAST 3216\n",
      "Salt_Pond_MetaG_R2_C_D2_MG_DASTool_bins_concoct_out.104.contigs__.RAST 3216\n",
      "Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.12.contigs__.RAST 3216\n",
      "Salt_Pond_MetaG_R2_restored_H2O_MG_DASTool_bins_maxbin.017.contigs__.RAST 4146\n",
      "Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.93.contigs__.RAST 530\n",
      "Salt_Pond_MetaG_R2A_C_D1_MG_DASTool_bins_concoct_out.15.contigs__.RAST 1176\n",
      "Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.10.contigs__.RAST 10806\n",
      "Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.51.contigs__.RAST 2510\n",
      "Salt_Pond_MetaGSF2_C_D2_MG_DASTool_bins_concoct_out.8.contigs__.RAST 4096\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "unfiltered_list = util.load(\"mag_list\")\n",
    "filter = [\n",
    "  \"Salt_Pond_MetaG_R2_restored_C_black_MG_DASTool_bins_concoct_out.17.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_concoct_out.52.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2_restored_DShore_MG_DASTool_bins_concoct_out.38.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2A_C_D2_MG_DASTool_bins_metabat.27.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R1_A_D2_MG_DASTool_bins_concoct_out.25.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2_B_D1_MG_DASTool_bins_concoct_out.110.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R1_A_D1_MG_DASTool_bins.metabat.47.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2A_A_D2_MG_DASTool_bins_concoct_out.10.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaGSF2_A_D2_MG_DASTool_bins_concoct_out.55.contigs__.RAST\",\n",
    "  \"Salt_Pond_MetaG_R2_A_D1_MG_DASTool_bins_metabat.32.contigs__.RAST\"\n",
    "]\n",
    "mag_list = []\n",
    "for item in unfiltered_list:\n",
    "    if item[1] in filter or len(filter) == 0:\n",
    "        mag_list.append(item)\n",
    "name_to_mag = util.load(\"name_to_mag\")\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "mag_probability_threshold = util.load(\"mag_probability_threshold\")\n",
    "#mag_protein_supplements = util.load(\"mag_protein_supplements\")\n",
    "for item in mag_list:\n",
    "#for name in problem_list:\n",
    "    full_data = util.get_object(item[1],item[7])\n",
    "    genome_obj = full_data[\"data\"]\n",
    "    genome_obj[\"assembly_ref\"] = str(item[6])+\"/\"+str(item[0])+\"/\"+str(item[4])+\";\"+genome_obj[\"assembly_ref\"]\n",
    "    name = mag_to_name[item[1]]\n",
    "    cds_hash = {}\n",
    "    for i,ftr in enumerate(genome_obj[\"cdss\"]):\n",
    "        cds_hash[ftr[\"id\"]] = ftr\n",
    "    ftr_hash = {}\n",
    "    for i,ftr in enumerate(genome_obj[\"features\"]):\n",
    "        ftr_hash[ftr[\"id\"]] = ftr\n",
    "        ftr[\"id\"] = name+\"_\"+str(i+1)\n",
    "        if \"cdss\" in ftr:\n",
    "            for j,cds in enumerate(ftr[\"cdss\"]):\n",
    "                cds_hash[cds][\"id\"] = ftr[\"id\"]+\".CDS\"\n",
    "                ftr[\"cdss\"][j] = cds_hash[cds][\"id\"]\n",
    "                cds_hash[cds][\"parent_gene\"] = ftr[\"id\"]\n",
    "    firstgene = genome_obj[\"features\"][0]\n",
    "    count = 0\n",
    "    if item[1] in mag_protein_supplements:\n",
    "        count = 1\n",
    "        total_genomes = mag_thresholds[item[1]][\"threshold_count\"]\n",
    "        for protein in mag_protein_supplements[item[1]]:\n",
    "            if mag_protein_supplements[item[1]][protein][0]/total_genomes >= mag_probability_threshold[item[1]]:\n",
    "                ftrid = name+'.pg_'+str(count)\n",
    "                count += 1\n",
    "                protseq = str(protein_hash[protein])\n",
    "                dnaseq = util.translate_protein_to_gene(protseq)\n",
    "                result = hashlib.md5(protseq.encode())\n",
    "                md5 = result.hexdigest()\n",
    "                result = hashlib.md5(dnaseq.encode())\n",
    "                dnamd5 = result.hexdigest()\n",
    "                newftr = {\n",
    "                    \"aliases\": [[\"MMseqMD5\",protein]],\n",
    "                    \"cdss\": [\n",
    "                        ftrid+\".CDS\"\n",
    "                    ],\n",
    "                    \"functions\":[\"Hypothetical protein\"],\n",
    "                    \"dna_sequence\": dnaseq,\n",
    "                    \"dna_sequence_length\": 3*len(protseq),\n",
    "                    \"id\": ftrid,\n",
    "                    \"location\": [\n",
    "                        [\n",
    "                            firstgene[\"location\"][0][0],\n",
    "                            1,\n",
    "                            \"+\",\n",
    "                            3*len(protseq)\n",
    "                        ]\n",
    "                    ],\n",
    "                    \"md5\": dnamd5,\n",
    "                    \"ontology_terms\": {},\n",
    "                    \"protein_md5\": md5,\n",
    "                    \"protein_translation\": protseq,\n",
    "                    \"protein_translation_length\": len(protseq),\n",
    "                    \"warnings\": []\n",
    "                }\n",
    "                cdsftr = newftr.copy()\n",
    "                del cdsftr[\"cdss\"]\n",
    "                cdsftr[\"id\"] = ftrid+\".CDS\"\n",
    "                cdsftr[\"parent_gene\"] = ftrid\n",
    "                genome_obj[\"features\"].append(newftr)\n",
    "                genome_obj[\"cdss\"].append(cdsftr)\n",
    "        print(item[1],count)\n",
    "        #Saving MAG\n",
    "        util.save(\"genome/\"+name,full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding and fixing redundant names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "name_to_mag = {}\n",
    "for item in mag_list:\n",
    "    if item[1] not in mag_to_name:\n",
    "        print(\"Missing:\",item[1])\n",
    "    elif mag_to_name[item[1]] in name_to_mag:\n",
    "        print(\"Collision:\",mag_to_name[item[1]])\n",
    "    else:\n",
    "        name_to_mag[mag_to_name[item[1]]] = item[1]\n",
    "util.save(\"name_to_mag\",name_to_mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading genomes to KBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "unfiltered_list = util.load(\"mag_list\")\n",
    "filter = []\n",
    "mag_list = []\n",
    "for item in unfiltered_list:\n",
    "    if item[1] in filter or len(filter) == 0:\n",
    "        mag_list.append(item)\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = datetime.timestamp(now)\n",
    "workspace = 187797\n",
    "anno = util.anno_client()\n",
    "anno.clients[\"GenomeFileUtil\"] = util.gfu_client()\n",
    "finished = util.load(\"finished_genomes\",[])\n",
    "unconverted = {}\n",
    "for item in mag_list:\n",
    "    name = mag_to_name[item[1]]\n",
    "    if name not in finished:\n",
    "        genome = util.load(\"annotated/\"+name)\n",
    "        count = 0\n",
    "        for ftr in genome[\"features\"]:\n",
    "            end = len(name)+3\n",
    "            if ftr[\"id\"][0:4] == \"Salt\":\n",
    "                unconverted[item[1]] = 1\n",
    "            if ftr[\"id\"][0:end] == name+\".pg\":\n",
    "                count += 1\n",
    "        print(name,count)\n",
    "        util.save_ws_object(name+\".pg\",187797,genome,\"KBaseGenomes.Genome\")\n",
    "        finished.append(name)\n",
    "        util.save(\"finished_genomes\",finished)\n",
    "for item in unconverted:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting genomes before load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "mags = util.msrecon.kbase_api.list_objects(187875, object_type=\"KBaseGenomes.Genome\", include_metadata=False)\n",
    "mag_hash = {}\n",
    "for item in mags:\n",
    "    mag_hash[item[1]] = item\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "for item in mag_list:\n",
    "    name = mag_to_name[item[1]]\n",
    "    name += \".pangenome.GLM4EC\"\n",
    "    if name in mag_hash:\n",
    "        print(\"Getting \"+name)\n",
    "        full_data = util.get_object(name,187875)\n",
    "        genome = full_data[\"data\"]\n",
    "        ftrhash = {}\n",
    "        for ftr in genome[\"features\"]:\n",
    "            ftrhash[ftr[\"id\"]] = ftr\n",
    "        for ftr in genome[\"cdss\"]:\n",
    "            ftrhash[ftr[\"id\"]] = ftr\n",
    "            if \"parent_gene\" in ftr:\n",
    "                if ftr[\"parent_gene\"] not in ftrhash:\n",
    "                    print(item[1],new_name,\"Gene \"+ftr[\"parent_gene\"]+\" not found!\")\n",
    "        for ftr in genome[\"features\"]:\n",
    "            if \"cdss\" in ftr:\n",
    "                for cds in ftr[\"cdss\"]:\n",
    "                    if cds not in ftrhash:\n",
    "                        print(item[1],new_name,\"CDS \"+cds+\" not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating genomes using RAST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /Users/chenry/code//kb_sdk/run_local/workdir/tmp/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1722232442.262796 ERROR: Requested data genome/R1_B_D1.20 doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/genome/R1_B_D1.20.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested data genome/R1_B_D1.20 doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/genome/R1_B_D1.20.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[1;32m     10\u001b[0m     rast_genome \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m:item[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenetic_code\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m11\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]\n\u001b[1;32m     17\u001b[0m     }\n\u001b[0;32m---> 18\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenome/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     genome \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ftr \u001b[38;5;129;01min\u001b[39;00m genome[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/code//chenry_utility_module/lib/chenry_utility_module/kbdevutils.py:147\u001b[0m, in \u001b[0;36mKBDevUtils.load\u001b[0;34m(self, name, default)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfilename)\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested data \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfilename))\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: Requested data genome/R1_B_D1.20 doesn't exist at /Users/chenry/code/notebooks/MicrobiomeNotebooks/Cliff/datacache/genome/R1_B_D1.20.json"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "from modelseedpy.core.rpcclient import RPCClient\n",
    "client = RPCClient(\"https://tutorial.theseed.org/services/genome_annotation\")\n",
    "for item in mag_list:\n",
    "    name = mag_to_name[item[1]]\n",
    "    if name not in output:\n",
    "        rast_genome = {\n",
    "            \"id\":item[1],\n",
    "            \"genetic_code\":11,\n",
    "            \"scientific_name\":\"Unknown\",\n",
    "            \"domain\":\"Bacteria\",\n",
    "            \"contigs\": [],\n",
    "            \"features\":[]\n",
    "        }\n",
    "        data = util.load(\"genome/\"+name)\n",
    "        genome = data[\"data\"]\n",
    "        for ftr in genome[\"features\"]:\n",
    "            rast_genome[\"features\"].append(ftr)\n",
    "        workflow = {\n",
    "            \"stages\":[\n",
    "                {\n",
    "                    \"name\": \"annotate_proteins_kmer_v2\", \n",
    "                    \"kmer_v2_parameters\": {}},\n",
    "                {\n",
    "                    \"name\": \"annotate_proteins_kmer_v1\",\n",
    "                    \"kmer_v1_parameters\": {\"annotate_hypothetical_only\": 1},},\n",
    "                {\n",
    "                    \"name\": \"annotate_proteins_similarity\",\n",
    "                    \"similarity_parameters\": {\"annotate_hypothetical_only\": 1},\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "        params = [{\"features\": rast_genome[\"features\"]}, workflow]\n",
    "        output = client.call(\"GenomeAnnotation.run_pipeline\",params)\n",
    "        util.save(\"rast/\"+name,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding RAST ontology event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = datetime.timestamp(now)\n",
    "for item in mag_list:\n",
    "    name = mag_to_name[item[1]]\n",
    "    data = util.load(\"genome/\"+name)\n",
    "    rast = util.load(\"rast/\"+name)\n",
    "    annoapi = util.anno_client(native_python_api=True)\n",
    "    events = [{\n",
    "        \"ontology_id\" : \"SSO\",\n",
    "        \"description\" : \"RAST Annotation API\",\n",
    "        \"method_version\" : \"1.0\",\n",
    "        \"method\" : \"annotate_genome\",\n",
    "        \"ontology_terms\": {},\n",
    "        \"timestamp\":\"2024-07-29T19:32:45\"\n",
    "    }]\n",
    "    for ftr in rast[0][\"features\"]:\n",
    "        if \"function\" in ftr:\n",
    "            events[0][\"ontology_terms\"][ftr[\"id\"]] = [{\"term\":ftr[\"function\"]}]\n",
    "    output = annoapi.add_annotation_ontology_events({\n",
    "        \"output_workspace\":187797,\n",
    "        \"events\":events,\n",
    "        \"overwrite_matching\":True,\n",
    "        \"object\":data[\"data\"],\n",
    "        \"type\":\"KBaseGenomes.Genome\",\n",
    "        \"input_ref\":None,\n",
    "        \"input_workspace\":None,\n",
    "        \"output_name\":None,\n",
    "        \"save\":0\n",
    "    })\n",
    "    util.save(\"annotated/\"+name,output[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing feature probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_thresholds = util.load(\"mag_thresholds\")\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "mag_protein_supplements = util.load(\"mag_protein_supplements\")\n",
    "feature_probabilities = {}\n",
    "for item in mag_list:\n",
    "    name = mag_to_name[item[1]]\n",
    "    data = util.load(\"annotated/\"+name)\n",
    "    feature_probabilities[item[1]] = {}\n",
    "    ftrs = data[\"features\"]\n",
    "    total_genomes = mag_thresholds[item[1]][\"threshold_count\"]\n",
    "    for ftr in ftrs:\n",
    "        if ftr[\"id\"].split(\".\")[-1][0:3] == \"pg_\":\n",
    "            feature_probabilities[item[1]][ftr[\"id\"]] = mag_protein_supplements[item[1]][ftr[\"aliases\"][0][1]][0]/total_genomes\n",
    "            if feature_probabilities[item[1]][ftr[\"id\"]] > 1:\n",
    "                feature_probabilities[item[1]][ftr[\"id\"]] = 1\n",
    "        else:\n",
    "            feature_probabilities[item[1]][ftr[\"id\"]] = 1\n",
    "util.save(\"feature_probabilities\",feature_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing metabolite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "metabolite_hash = util.load(\"met_NameMSID\")\n",
    "\n",
    "from numpy import array\n",
    "newMets = array([\"2'-Deoxyuridine\", '2-Oxoglutarate', '2-Oxoisocaproate', '3-Hydroxybutyrate', '3-Hydroxyisovalerate', '3-Methyl-2-oxovalerate', '4-Aminobutyrate', 'Acetate', 'Acetone', 'Alanine', 'Arginine', 'Aspartate', 'Benzoate', 'Betaine', 'Dimethylamine', 'Ethanol', 'Formate', 'Fructose', 'Fumarate', 'Glucose', 'Glutamate', 'Glycerol', 'Isobutyrate', 'Isoleucine', 'Isopropanol', 'Isovalerate', 'Lactate', 'Leucine', 'Maltose', 'Methanol', 'Methionine', 'Methylamine', 'Methylguanidine', 'N,N-Dimethylglycine', 'Phenylacetate', 'Phenylalanine', 'Proline', 'Propionate', 'Propylene glycol', 'Pyroglutamate', 'Succinate', 'Sucrose', 'Thymidine', 'Trehalose', 'Trimethylamine', 'Tryptophan', 'Tyrosine', 'Uracil', 'Uridine', 'Valine'])\n",
    "print(set(newMets)-set(list(metabolite_hash.keys())))\n",
    "\n",
    "metabolite_names = {cpdID: metName for metName, cpdID in metabolite_hash.items()}\n",
    "util.save(\"metabolite_names\",metabolite_names)\n",
    "util.save(\"metabolites\",list(metabolite_names.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing SMIPPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files printed to:/scratch/shared/code/MicrobiomeNotebooks/Cliff/nboutput when using KBDevUtils.output_dir\n",
      "ModelSEED: /scratch/shared//sdkmount/run_local/workdir/tmp/\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='kbase.us', port=443): Max retries exceeded with url: /services/ws (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe57732fdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connection.py:158\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/util/connection.py:80\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/util/connection.py:70\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 70\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connectionpool.py:597\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connectionpool.py:343\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connectionpool.py:839\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connection.py:301\u001b[0m, in \u001b[0;36mVerifiedHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connection.py:167\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7fe57732fdd0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/connectionpool.py:637\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    635\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m--> 637\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/urllib3/util/retry.py:399\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    401\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='kbase.us', port=443): Max retries exceeded with url: /services/ws (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe57732fdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mag \u001b[38;5;129;01min\u001b[39;00m mag_list:\n\u001b[1;32m    178\u001b[0m     mag[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m mag_to_name[mag[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pg.G.D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 180\u001b[0m auxo_media \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsrecon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_media\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_ws\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/AuxoMedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m gmm_base_media \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mmsrecon\u001b[38;5;241m.\u001b[39mget_media(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_ws\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/PyruateMinimalAerobic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# run the parallelized code\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/shared/code/KBBaseModules/kbbasemodules/basemodelingmodule.py:139\u001b[0m, in \u001b[0;36mBaseModelingModule.get_media\u001b[0;34m(self, id_or_ref, ws)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_media\u001b[39m(\u001b[38;5;28mself\u001b[39m,id_or_ref,ws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 139\u001b[0m     media \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkbase_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_from_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_or_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     media\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m media\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_objects\u001b[38;5;241m.\u001b[39mappend(media\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mreference)\n",
      "File \u001b[0;32m/scratch/shared/code/cobrakbase/cobrakbase/kbaseapi.py:220\u001b[0m, in \u001b[0;36mKBaseAPI.get_from_ws\u001b[0;34m(self, id_or_ref, workspace)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_from_ws\u001b[39m(\u001b[38;5;28mself\u001b[39m, id_or_ref, workspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 220\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobjects\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_workspace_identifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_or_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/shared/code/cobrakbase/cobrakbase/kbaseapi.py:190\u001b[0m, in \u001b[0;36mKBaseAPI.get_objects2\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tries \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mws_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects2\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ServerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m32400\u001b[39m:\n",
      "File \u001b[0;32m/scratch/shared/code/chenry_utility_module/lib/installed_clients/WorkspaceClient.py:1959\u001b[0m, in \u001b[0;36mWorkspace.get_objects2\u001b[0;34m(self, params, context)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_objects2\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;124;03m    Get objects from the workspace.\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;124;03m    :param params: instance of type \"GetObjects2Params\" (Input parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;124;03m       \"handle_stacktrace\" of String\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWorkspace.get_objects2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/shared/code/chenry_utility_module/lib/installed_clients/baseclient.py:282\u001b[0m, in \u001b[0;36mBaseClient.call_method\u001b[0;34m(self, service_method, args, service_ver, context)\u001b[0m\n\u001b[1;32m    280\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_service_url(service_method, service_ver)\n\u001b[1;32m    281\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_context(service_ver, context)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/shared/code/chenry_utility_module/lib/installed_clients/baseclient.py:179\u001b[0m, in \u001b[0;36mBaseClient._call\u001b[0;34m(self, url, method, params, context)\u001b[0m\n\u001b[1;32m    176\u001b[0m     arg_hash[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m context\n\u001b[1;32m    178\u001b[0m body \u001b[38;5;241m=\u001b[39m _json\u001b[38;5;241m.\u001b[39mdumps(arg_hash, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m_JSONObjectEncoder)\n\u001b[0;32m--> 179\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_requests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_all_ssl_certificates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m ret\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/api.py:116\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/api.py:60\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/sessions.py:533\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    531\u001b[0m }\n\u001b[1;32m    532\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 533\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/sessions.py:646\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/env/python3_modelseed/lib/python3.11/site-packages/requests/adapters.py:516\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='kbase.us', port=443): Max retries exceeded with url: /services/ws (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe57732fdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "%run cliffcommutil.py\n",
    "\n",
    "def smipps(arguments):\n",
    "    # unpack the arguments\n",
    "    (mag_list, mags_to_models, model_ws, metabolites, feature_probabilities,problemlist, auxo_media, gmm_base_media, aerobicity\n",
    "    ) = arguments\n",
    "    \n",
    "    # print status\n",
    "    from multiprocess import current_process\n",
    "    pid = current_process().name\n",
    "    print(pid)\n",
    "    \n",
    "    # define the phenotype sets\n",
    "    uptake_phenoset = util.create_phenotypeset_from_compounds(\n",
    "        metabolites,\n",
    "        base_media=auxo_media,\n",
    "        base_uptake=0,\n",
    "        base_excretion=1000,\n",
    "        global_atom_limits={},\n",
    "        type=\"uptake\"\n",
    "    )\n",
    "    excretion_phenoset = util.create_phenotypeset_from_compounds(\n",
    "        metabolites,\n",
    "        base_media=auxo_media,\n",
    "        base_uptake=0,\n",
    "        base_excretion=1000,\n",
    "        global_atom_limits={},\n",
    "        type=\"excretion\"\n",
    "    )\n",
    "    growth_phenoset = util.create_phenotypeset_from_compounds(\n",
    "        metabolites,\n",
    "        base_media=gmm_base_media,\n",
    "        base_uptake=0,\n",
    "        base_excretion=1000,\n",
    "        global_atom_limits={},\n",
    "        type=\"growth\"\n",
    "    )\n",
    "    phenosets = {\"uptake\":uptake_phenoset,\"excretion\":excretion_phenoset,\"growth\":growth_phenoset}\n",
    "    \n",
    "    # compute the SMIPP for each MAG\n",
    "    for i,mag in enumerate(mag_list):\n",
    "        print(mag)\n",
    "        name = mag[1]\n",
    "        if name not in probability_finished and name not in problemlist:\n",
    "            mdlutl = util.msrecon.get_model(name+model_suffix,model_ws)\n",
    "            reaction_probabilities[name] = {}\n",
    "            for rxn in mdlutl.model.reactions:\n",
    "                highest_prob = None\n",
    "                for gene in rxn.genes:\n",
    "                    if gene.id in feature_probabilities[mag[1]]:\n",
    "                        if highest_prob == None or feature_probabilities[mag[1]][gene.id] > highest_prob:\n",
    "                            highest_prob = feature_probabilities[mag[1]][gene.id]\n",
    "                if highest_prob != None:\n",
    "                    rxn.probability = highest_prob\n",
    "                    reaction_probabilities[name][rxn.id] = highest_prob\n",
    "\n",
    "            print(mdlutl.model.genome_ref)\n",
    "            genome = util.msrecon.get_msgenome_from_ontology(mdlutl.model.genome_ref,native_python_api=True,output_ws=None)\n",
    "            reaction_hash = genome.annoont.get_reaction_gene_hash(feature_type=\"gene\")\n",
    "            for rxn in reaction_hash:\n",
    "                highest_prob = None\n",
    "                for gene in reaction_hash[rxn]:\n",
    "                    if gene in feature_probabilities[mag[1]]:\n",
    "                        if highest_prob == None or feature_probabilities[mag[1]][gene] > highest_prob:\n",
    "                            highest_prob = feature_probabilities[mag[1]][gene]\n",
    "                if highest_prob != None and (rxn+\"_c0\" not in reaction_probabilities[name] or highest_prob >= reaction_probabilities[name][rxn+\"_c0\"]):\n",
    "                    if rxn+\"_c0\" in mdlutl.model.reactions:\n",
    "                        mdlutl.model.reactions.get_by_id(rxn+\"_c0\").probability = highest_prob\n",
    "                    reaction_probabilities[name][rxn+\"_c0\"] = highest_prob\n",
    "\n",
    "            filters = mdlutl.get_attributes(\"gf_filter\")\n",
    "            tests = mdlutl.get_atp_tests(core_template=util.msrecon.core_template,atp_media_filename=util.msrecon.module_dir+\"/data/atp_medias.tsv\",recompute=False)\n",
    "            msgapfill = MSGapfill(\n",
    "                mdlutl,\n",
    "                [util.msrecon.get_template(mdlutl.model.template_ref)],\n",
    "                [],\n",
    "                tests,\n",
    "                blacklist=[],\n",
    "                default_target=\"bio1\",\n",
    "                minimum_obj=0.01,\n",
    "                base_media=None,\n",
    "                base_media_target_element=None\n",
    "            )\n",
    "\n",
    "            #Adding missing transporter for metabolites to gapfilling database\n",
    "            for cpd in metabolites:\n",
    "                if \"EX_\"+cpd+\"_e0\" not in msgapfill.gfmodelutl.model.reactions:\n",
    "                    transport = msgapfill.gfmodelutl.add_transport_and_exchange_for_metabolite(cpd,direction=\"=\",prefix=\"trans\",override=False)\n",
    "\n",
    "            coefficients = {}\n",
    "            gf_penalties = msgapfill.gfpkgmgr.getpkg(\"GapfillingPkg\").gapfilling_penalties\n",
    "            gfrxn = 0\n",
    "            probrxn = 0\n",
    "            otherrxn = 0\n",
    "            for reaction in msgapfill.gfmodelutl.model.reactions:\n",
    "                if reaction.id in reaction_probabilities[name]:\n",
    "                    probrxn += 2\n",
    "                    coefficients[\">\"+reaction.id] = 1-reaction_probabilities[name][reaction.id]\n",
    "                    coefficients[\"<\"+reaction.id] = 1-reaction_probabilities[name][reaction.id]\n",
    "                elif reaction.id in gf_penalties:\n",
    "                    if \"forward\" in gf_penalties[reaction.id]:\n",
    "                        gfrxn += 1\n",
    "                        coefficients[\">\"+reaction.id] = 1+gf_penalties[reaction.id][\"forward\"]\n",
    "                    else:\n",
    "                        otherrxn += 1\n",
    "                        coefficients[\">\"+reaction.id] = 0.95\n",
    "                    if \"reverse\" in gf_penalties[reaction.id]:\n",
    "                        gfrxn += 1\n",
    "                        coefficients[\"<\"+reaction.id] = 1+gf_penalties[reaction.id][\"reverse\"]\n",
    "                    else:\n",
    "                        otherrxn += 1\n",
    "                        coefficients[\"<\"+reaction.id] = 0.95\n",
    "                else:\n",
    "                    otherrxn += 2\n",
    "                    coefficients[\">\"+reaction.id] = 0.95\n",
    "                    coefficients[\"<\"+reaction.id] = 0.95\n",
    "            print(name,\"GF:\",gfrxn,\"Prob:\",probrxn,\"Other:\",otherrxn)\n",
    "\n",
    "            # Create conditional logic for growth phenotypes if errors are raised here \n",
    "            msgapfill.prefilter(test_conditions=tests,growth_conditions=[],use_prior_filtering=True,base_filter_only=True)\n",
    "\n",
    "            gf_phenotype_results[name] = {}\n",
    "            for phenoid in phenosets:\n",
    "                gf_phenotype_results[name][phenoid] = {}\n",
    "                output = phenosets[phenoid].simulate_phenotypes(\n",
    "                    msgapfill.gfmodelutl,\n",
    "                    multiplier=2,\n",
    "                    add_missing_exchanges=True,\n",
    "                    save_fluxes=False,\n",
    "                    save_reaction_list=True,\n",
    "                    gapfill_negatives=False,\n",
    "                    msgapfill=None,\n",
    "                    test_conditions=None,\n",
    "                    ignore_experimental_data=True,\n",
    "                    flux_coefficients=coefficients\n",
    "                )\n",
    "                for index, row in output[\"details\"].iterrows():\n",
    "                    if \"reactions\" in output[\"data\"][row[\"Phenotype\"]]:\n",
    "                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] = 0\n",
    "                        for rxn in output[\"data\"][row[\"Phenotype\"]][\"reactions\"]:\n",
    "                            direction = rxn[0:1]\n",
    "                            rxnid = rxn[1:]\n",
    "                            if direction == \">\":\n",
    "                                if rxnid not in gf_penalties or \"forward\" not in gf_penalties[rxnid]:\n",
    "                                    if rxnid in reaction_probabilities[name]:\n",
    "                                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] += reaction_probabilities[name][rxnid]\n",
    "                                    else:\n",
    "                                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] += 0.05\n",
    "                            elif direction == \"<\":\n",
    "                                if rxnid not in gf_penalties or \"reverse\" not in gf_penalties[rxnid]:\n",
    "                                    if rxnid in reaction_probabilities[name]:\n",
    "                                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] += reaction_probabilities[name][rxnid]\n",
    "                                    else:\n",
    "                                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] += 0.05\n",
    "                        output[\"data\"][row[\"Phenotype\"]][\"average_probability\"] = output[\"data\"][row[\"Phenotype\"]][\"average_probability\"]/len(output[\"data\"][row[\"Phenotype\"]][\"reactions\"])\n",
    "                    gf_phenotype_results[name][phenoid][row[\"Phenotype\"]] = output[\"data\"][row[\"Phenotype\"]]\n",
    "            probability_finished.append(name)\n",
    "            util.save(\"new_gf_phenotype_results_\",gf_phenotype_results)\n",
    "            util.save(\"reaction_probabilities\",reaction_probabilities)\n",
    "            util.save(\"probability_finished_\",probability_finished)\n",
    "        break\n",
    "                \n",
    "                \n",
    "# define function parameters\n",
    "mag_list = util.load(\"mag_list\")\n",
    "model_ws = 188065\n",
    "model_suffix = \".pyr\"\n",
    "metabolites = util.load(\"metabolites\")\n",
    "feature_probabilities = util.load(\"feature_probabilities\")\n",
    "reaction_probabilities = util.load(\"reaction_probabilities\",{})\n",
    "gf_phenotype_results = util.load(\"new_gf_phenotype_results_\",{})\n",
    "probability_finished = util.load(\"probability_finished_\",[])\n",
    "problemlist = util.load(\"problemlist\",[])\n",
    "mag_to_name = util.load(\"mag_to_name\")\n",
    "mag_feature_probabilities = {mag_to_name[k]+\".pg.G.D\":v for k,v in feature_probabilities.items()}\n",
    "# print(mag_feature_probabilities.keys())\n",
    "for mag in mag_list:\n",
    "    mag[1] = mag_to_name[mag[1]]+\".pg.G.D\"\n",
    "\n",
    "auxo_media = util.msrecon.get_media(f\"{model_ws}/AuxoMedia\")\n",
    "gmm_base_media = util.msrecon.get_media(f\"{model_ws}/PyruateMinimalAerobic\")\n",
    "\n",
    "\n",
    "# run the parallelized code\n",
    "parallelize = True\n",
    "if parallelize:\n",
    "    from multiprocess import Pool\n",
    "    from os import cpu_count\n",
    "    from numpy import array_split\n",
    "    \n",
    "    # the MAGs are partitioned into groups of 4, and a thread is created to handle each group \n",
    "    mag_lists = array_split(mag_list, int(len(mag_list)/4))\n",
    "    pool = Pool(min(cpu_count(), len(mag_lists)))\n",
    "    args = [(mag_list, mag_to_name, model_ws, metabolites, mag_feature_probabilities, problemlist, auxo_media, gmm_base_media, \"aerobic\")\n",
    "           for mag_list in mag_lists]\n",
    "    list_of_outputs = pool.map(smipps, args)\n",
    "else:\n",
    "    outputs = smipps((mag_list, mag_to_name, model_ws, metabolites, mag_feature_probabilities, problemlist, auxo_media, gmm_base_media, \"aerobic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating gapfilling phenotype data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "consolidated_gf_results = util.load(\"consolidated_gf_results\",{})\n",
    "reaction_probabilities = util.load(\"reaction_probabilities\",{})\n",
    "for i in range(0,8,1):\n",
    "    phenotype_gf_results = util.load(\"gf_phenotype_results_\"+str(i))\n",
    "    part_reaction_probabilities = util.load(\"reaction_probabilities\"+str(i))\n",
    "    for name in phenotype_gf_results:\n",
    "        consolidated_gf_results[name] = phenotype_gf_results[name]\n",
    "         reaction_probabilities[name] = part_reaction_probabilities[name]\n",
    "util.save(\"consolidated_gf_results\",consolidated_gf_results)\n",
    "util.save(\"reaction_probabilities\",reaction_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing proper gapfill and filtering bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "consolidated_gf_results = util.load(\"consolidated_gf_results\")\n",
    "reaction_probabilities = util.load(\"reaction_probabilities\")\n",
    "mag_to_model = util.load(\"mag_to_model\")\n",
    "studies = [\"uptake\",\"excretion\",\"growth\"]\n",
    "for name in consolidated_gf_results:\n",
    "    modelname = mag_to_model(name)\n",
    "    #Getting base model\n",
    "    mdlutl = util.msrecon.get_model(modelname,186678)\n",
    "    \n",
    "    for study in studies:\n",
    "        if study not in consolidated_gf_results[name]:\n",
    "            continue\n",
    "        for phenotype in consolidated_gf_results[name][study]:\n",
    "            if \"reactions\" in consolidated_gf_results[name][study][phenotype]:\n",
    "                consolidated_gf_results[name][study][phenotype][\"new_probability\"] = 0\n",
    "                consolidated_gf_results[name][study][phenotype][\"probrxn\"] = 0\n",
    "                consolidated_gf_results[name][study][phenotype][\"otherrxn\"] = 0\n",
    "                consolidated_gf_results[name][study][phenotype][\"origgfrxn\"] = 0\n",
    "                consolidated_gf_results[name][study][phenotype][\"new_gf\"] = 0\n",
    "                consolidated_gf_results[name][study][phenotype][\"new_gf_reactions\"] = []\n",
    "                found = False\n",
    "                for rxn in consolidated_gf_results[name][study][phenotype][\"reactions\"]:\n",
    "                    direction = rxn[0:1]\n",
    "                    rxnid = rxn[1:]\n",
    "                    if rxnid[0:3] == \"EX_\" and rxnid[3:11] == phenotype:\n",
    "                        found = True\n",
    "                    if rxnid not in mdlutl.model.reactions:\n",
    "                        consolidated_gf_results[name][study][phenotype][\"new_gf_reactions\"].append(rxn)\n",
    "                        consolidated_gf_results[name][study][phenotype][\"new_gf\"] += 1\n",
    "                    elif direction == \">\":\n",
    "                        if mdlutl.model.reactions.get_by_id(rxnid).upper_bound <= 0:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_gf_reactions\"].append(rxn)\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_gf\"] += 1\n",
    "                        elif rxnid in reaction_probabilities[name]:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_probability\"] += reaction_probabilities[name][rxnid]\n",
    "                            consolidated_gf_results[name][study][phenotype][\"probrxn\"] += 1\n",
    "                        elif rxnid[0:3] != \"bio\" and rxnid[0:3] != \"EX_\" and rxnid[0:3] != \"DM_\" and len(mdlutl.model.reactions.get_by_id(rxnid).genes) == 0:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"origgfrxn\"] += 1\n",
    "                        else:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"otherrxn\"] += 1\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_probability\"] += 0.05  \n",
    "                    elif direction == \"<\":\n",
    "                        if mdlutl.model.reactions.get_by_id(rxnid).lower_bound >= 0:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_gf_reactions\"].append(rxn)\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_gf\"] += 1\n",
    "                        elif rxnid in reaction_probabilities[name]:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_probability\"] += reaction_probabilities[name][rxnid]\n",
    "                            consolidated_gf_results[name][study][phenotype][\"probrxn\"] += 1\n",
    "                        elif rxnid[0:3] != \"bio\" and rxnid[0:3] != \"EX_\" and rxnid[0:3] != \"DM_\" and len(mdlutl.model.reactions.get_by_id(rxnid).genes) == 0:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"origgfrxn\"] += 1\n",
    "                        else:\n",
    "                            consolidated_gf_results[name][study][phenotype][\"otherrxn\"] += 1\n",
    "                            consolidated_gf_results[name][study][phenotype][\"new_probability\"] += 0.05  \n",
    "                totalrxn = consolidated_gf_results[name][study][phenotype][\"probrxn\"]+consolidated_gf_results[name][study][phenotype][\"new_gf\"]+consolidated_gf_results[name][study][phenotype][\"otherrxn\"]\n",
    "                consolidated_gf_results[name][study][phenotype][\"new_probability\"] = consolidated_gf_results[name][study][phenotype][\"new_probability\"]/totalrxn\n",
    "\n",
    "                if not found:\n",
    "                    consolidated_gf_results[name][study][phenotype][\"original_objective_value\"] = consolidated_gf_results[name][study][phenotype][\"objective_value\"]\n",
    "                    consolidated_gf_results[name][study][phenotype][\"objective_value\"] = 0\n",
    "                    consolidated_gf_results[name][study][phenotype][\"class\"] = \"N\"\n",
    "                    consolidated_gf_results[name][study][phenotype][\"positive\"] = False\n",
    "                    consolidated_gf_results[name][study][phenotype][\"new_probability\"] = 0\n",
    "util.save(\"consolidated_gf_results\",consolidated_gf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "consolidated_gf_results = util.load(\"consolidated_gf_results\")\n",
    "metabolites = util.load(\"metabolites\")\n",
    "metabolite_names = util.load(\"metabolite_names\")\n",
    "mag_abundances = util.load(\"mag_abundances\")\n",
    "total_mag_abundances = util.load(\"total_mag_abundances\")\n",
    "records = {\n",
    "    \"uptake_prob\":[{\"Sample\":\"Name\"}],\n",
    "    \"excretion_prob\":[{\"Sample\":\"Name\"}],\n",
    "    \"growth_prob\":[{\"Sample\":\"Name\"}],\n",
    "}\n",
    "types = [\"uptake\",\"excretion\",\"growth\"]\n",
    "for met in metabolites:\n",
    "    for record in records:\n",
    "        records[record][0][met] = metabolite_names[met]\n",
    "for sample in mag_abundances:\n",
    "    new_record = {\n",
    "        \"uptake_prob\":{\"Sample\":sample},\n",
    "        \"excretion_prob\":{\"Sample\":sample},\n",
    "        \"growth_prob\":{\"Sample\":sample}\n",
    "    }\n",
    "    for metabolite in metabolites:\n",
    "        for record in new_record:\n",
    "            new_record[record][metabolite] = 0\n",
    "    for name in mag_abundances[sample]:\n",
    "        for metabolite in metabolites:\n",
    "            for currtype in types:\n",
    "                if currtype in consolidated_gf_results[name]:\n",
    "                    if metabolite in consolidated_gf_results[name][currtype] and \"new_probability\" in consolidated_gf_results[name][currtype][metabolite]:\n",
    "                        new_record[currtype+\"_prob\"][metabolite] += mag_abundances[sample][asvset]/total_mag_abundances[sample]*consolidated_gf_results[name][currtype][metabolite][\"new_probability\"]\n",
    "    for record in records:\n",
    "        records[record].append(new_record[record])\n",
    "#Printing interation matrices\n",
    "for record in records:\n",
    "    df = pd.DataFrame.from_records(records[record])\n",
    "    df.to_csv(util.output_dir+\"/Sample\"+record+\"Interactions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing ASV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "consolidated_gf_results = util.load(\"consolidated_gf_results\")\n",
    "metabolites = util.load(\"metabolites\")\n",
    "metabolite_names = util.load(\"metabolite_names\")\n",
    "mag_list = util.load(\"mag_list\")\n",
    "records = {\n",
    "    \"uptake_prob\":[{\"MAG\":\"Name\"}],\n",
    "    \"excretion_prob\":[{\"MAG\":\"Name\"}],\n",
    "    \"growth_prob\":[{\"MAG\":\"Name\"}],\n",
    "    \"uptake_data\":[{\"MAG\":\"Name\"}],\n",
    "    \"excretion_data\":[{\"MAG\":\"Name\"}],\n",
    "    \"growth_data\":[{\"MAG\":\"Name\"}],\n",
    "}\n",
    "types = [\"uptake\",\"excretion\",\"growth\"]\n",
    "for met in metabolites:\n",
    "    for record in records:\n",
    "        records[record][0][met] = metabolite_names[met]\n",
    "for mag in mag_list:\n",
    "    name = mag[1]\n",
    "    if name not in consolidated_gf_results:\n",
    "        print(\"No data for \",name)\n",
    "        continue\n",
    "    new_record = {}\n",
    "    for currtype in types:\n",
    "        new_record[currtype+\"_prob\"] = {\"MAG\":name}\n",
    "        new_record[currtype+\"_data\"] = {\"MAG\":name}\n",
    "    for metabolite in metabolites:\n",
    "        for currtype in types:\n",
    "            if currtype in consolidated_gf_results[name]:\n",
    "                if metabolite in consolidated_gf_results[name][currtype] and \"new_probability\" in consolidated_gf_results[name][currtype][metabolite]:\n",
    "                    new_record[currtype+\"_prob\"][metabolite] = consolidated_gf_results[name][currtype][metabolite][\"new_probability\"]\n",
    "                    new_record[currtype+\"_data\"][metabolite] = str(consolidated_gf_results[name][currtype][metabolite][\"probrxn\"])\n",
    "                    new_record[currtype+\"_data\"][metabolite] += \"/\"+str(consolidated_gf_results[name][currtype][metabolite][\"otherrxn\"])\n",
    "                    new_record[currtype+\"_data\"][metabolite] += \"/\"+str(consolidated_gf_results[name][currtype][metabolite][\"origgfrxn\"])\n",
    "                    new_record[currtype+\"_data\"][metabolite] += \"/\"+str(consolidated_gf_results[name][currtype][metabolite][\"new_gf\"])\n",
    "                    new_record[currtype+\"_data\"][metabolite] += \"/\"+str(consolidated_gf_results[name][currtype][metabolite][\"gapfill_count\"])\n",
    "                    new_record[currtype+\"_data\"][metabolite] += \"/\"+str(consolidated_gf_results[name][currtype][metabolite][\"reaction_count\"])\n",
    "    for record in records:\n",
    "        records[record].append(new_record[record])\n",
    "#Printing interation matrices\n",
    "for record in records:\n",
    "    df = pd.DataFrame.from_records(records[record])\n",
    "    df.to_csv(util.output_dir+\"/MAG\"+record+\"Interactions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gapfilling models based on gapfilling simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "import numpy as np\n",
    "consolidated_gf_results = util.load(\"consolidated_gf_results\")\n",
    "finished_models = util.load(\"finished_models\",{})\n",
    "metabolites = util.load(\"metabolites\")\n",
    "reactions_to_add = {}\n",
    "\n",
    "for name in consolidated_gf_results:\n",
    "    if name not in finished_models:\n",
    "        mdlutl = util.msrecon.get_model(name+\".ASV.mdl\",181152)\n",
    "        types = [\"uptake\",\"excretion\",\"growth\"]\n",
    "        reactions_to_add[name] = {}\n",
    "        for currtype in types:\n",
    "            problist = []\n",
    "            for metabolite in metabolites:\n",
    "                if metabolite in consolidated_gf_results[name][currtype] and \"new_probability\" in consolidated_gf_results[name][currtype][metabolite]:\n",
    "                    if consolidated_gf_results[name][currtype][metabolite][\"new_probability\"] > 0:\n",
    "                        problist.append(consolidated_gf_results[name][currtype][metabolite][\"new_probability\"])\n",
    "            if len(problist) > 0:\n",
    "                ave = np.array(problist).mean()\n",
    "                stdev = np.array(problist).std()\n",
    "                for metabolite in metabolites:\n",
    "                    if metabolite in consolidated_gf_results[name][currtype] and \"new_probability\" in consolidated_gf_results[name][currtype][metabolite]:\n",
    "                        if consolidated_gf_results[name][currtype][metabolite][\"new_probability\"] > 0:\n",
    "                            consolidated_gf_results[name][currtype][metabolite][\"zscore\"] = (consolidated_gf_results[name][currtype][metabolite][\"new_probability\"]-ave)/stdev\n",
    "                            if consolidated_gf_results[name][currtype][metabolite][\"zscore\"] >= -1 and \"reactions\" in consolidated_gf_results[name][currtype][metabolite]:\n",
    "                                for rxn in consolidated_gf_results[name][currtype][metabolite][\"reactions\"]:\n",
    "                                    direction = rxn[0:1]\n",
    "                                    rxnid = rxn[1:]\n",
    "                                    if rxnid not in mdlutl.model.reactions:\n",
    "                                        if rxnid not in reactions_to_add[name]:\n",
    "                                            reactions_to_add[name][rxnid] = {}\n",
    "                                        reactions_to_add[name][rxnid][direction] = consolidated_gf_results[name][currtype][metabolite][\"zscore\"]\n",
    "                                    elif direction == \">\":\n",
    "                                        if mdlutl.model.reactions.get_by_id(rxnid).upper_bound <= 0:\n",
    "                                            if rxnid not in reactions_to_add[name]:\n",
    "                                                reactions_to_add[name][rxnid] = {}\n",
    "                                            reactions_to_add[name][rxnid][direction] = consolidated_gf_results[name][currtype][metabolite][\"zscore\"]\n",
    "                                    elif direction == \"<\":\n",
    "                                        if mdlutl.model.reactions.get_by_id(rxnid).lower_bound >= 0:\n",
    "                                            if rxnid not in reactions_to_add[name]:\n",
    "                                                reactions_to_add[name][rxnid] = {}\n",
    "                                            reactions_to_add[name][rxnid][direction] = consolidated_gf_results[name][currtype][metabolite][\"zscore\"]\n",
    "        util.save(\"consolidated_gf_results\",consolidated_gf_results)\n",
    "        msgapfill = MSGapfill(\n",
    "            mdlutl,\n",
    "            [util.msrecon.get_template(mdlutl.model.template_ref)],\n",
    "            [],\n",
    "            [],\n",
    "            blacklist=[],\n",
    "            default_target=\"bio1\",\n",
    "            minimum_obj=0.01,\n",
    "            base_media=None,\n",
    "            base_media_target_element=None\n",
    "        )\n",
    "        for cpd in metabolites:\n",
    "            if \"EX_\"+cpd+\"_e0\" not in msgapfill.gfmodelutl.model.reactions:\n",
    "                transport = msgapfill.gfmodelutl.add_transport_and_exchange_for_metabolite(cpd,direction=\"=\",prefix=\"trans\",override=False)\n",
    "        for rxnid in reactions_to_add[name]:\n",
    "            if rxnid in msgapfill.gfmodel.reactions:\n",
    "                rxn = msgapfill.gfmodel.reactions.get_by_id(rxnid)\n",
    "                rxn = rxn.copy()\n",
    "                mdlutl.model.add_reactions([rxn])\n",
    "                rxn.upper_bound = 0\n",
    "                rxn.lower_bound = 0\n",
    "                if \">\" in reactions_to_add[name][rxnid]:\n",
    "                    rxn.upper_bound = 100\n",
    "                if \"<\" in reactions_to_add[name][rxnid]:\n",
    "                    rxn.lower_bound = -100\n",
    "            else:\n",
    "                print(name,\"Missing reaction\",rxnid,reactions_to_add[name][rxnid])\n",
    "        util.msrecon.save_model(mdlutl,181152,name+\".ASV.mdl.gf\")\n",
    "        finished_models[name] = 1\n",
    "    util.save(\"finished_models\",finished_models)\n",
    "util.save(\"reactions_to_add\",reactions_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and saving community model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "import cobra\n",
    "mag_list = util.load(\"mag_list\")\n",
    "mag_abundances = util.load(\"mag_abundances\")\n",
    "total_mag_abundances = util.load(\"total_mag_abundances\")\n",
    "\n",
    "for sample in mag_abundances:\n",
    "    member_models = []\n",
    "    member_names = []\n",
    "    member_abundances = {}\n",
    "    asv_hash = {}\n",
    "    for mag in mag_abundances[sample]:\n",
    "        if mag_abundances[sample][mag]/total_mag_abundances[sample] > 0.01:\n",
    "            mdlutl = util.msrecon.get_model(name+\".ASV.mdl.gf\",181152)\n",
    "            member_models.append(mdlutl.model)\n",
    "            member_names.append(name)\n",
    "            member_abundances[name] = mag_abundances[sample][mag]/total_mag_abundances[sample]\n",
    "    comm_model = MSCommunity.build_from_species_models(\n",
    "        member_models,\n",
    "        mdlid=sample,\n",
    "        name=sample,\n",
    "        names=member_names,\n",
    "        abundances=member_abundances\n",
    "    )\n",
    "    cobra.io.save_json_model(comm_model.model, \"models/\"+sample+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running community model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cliffcommutil.py\n",
    "procid = 0\n",
    "metabolites = util.load(\"metabolites\")\n",
    "metabolomics_data = util.load(\"metabolomics_data\")\n",
    "feature_probabilities = util.load(\"feature_probabilities\")\n",
    "community_model_data = util.load(\"community_model_data\")\n",
    "from optlang.symbolics import Zero, add\n",
    "import cobra\n",
    "\n",
    "complete_media = util.msrecon.get_media(\"KBaseMedia/Complete\")\n",
    "\n",
    "model_list = [\"RC-ABX_-1.5\",\"RC-ABX_1.5\",\"RC-ABX_4.0\",\"RC-ABX_6.0\",\"RC-ABX_9.0\",\"RC-ABX_12.5\"]\n",
    "output = {}\n",
    "rxn_record_hash = {}\n",
    "records = [\n",
    "    {\"id\":\"max_growth\"},\n",
    "    {\"id\":\"carbon_uptake\"},\n",
    "    {\"id\":\"flux fitting objective\"},\n",
    "    {\"id\":\"minimum probability objective\"},\n",
    "]\n",
    "for modelid in model_list:\n",
    "    output[modelid] = {}\n",
    "    base_model = cobra.io.load_json_model(\"models/\"+modelid+'.json')\n",
    "    current_comm_model = MSCommunity(\n",
    "        model=base_model,\n",
    "        names=community_model_data[modelid][\"names\"]\n",
    "    )\n",
    "    #loadedmodel.solver = 'gurobi'\n",
    "    dipeptide_exchanges = [\n",
    "        \"EX_cpd11591_e0\",\n",
    "        \"EX_cpd11589_e0\",\n",
    "        \"EX_cpd15605_e0\",\n",
    "        \"EX_cpd11588_e0\",\n",
    "        \"EX_cpd11583_e0\",\n",
    "        \"EX_cpd11580_e0\",\n",
    "        \"EX_cpd11593_e0\",\n",
    "        \"EX_cpd11585_e0\",\n",
    "        \"EX_cpd11586_e0\",\n",
    "        \"EX_cpd15604_e0\",\n",
    "        \"EX_cpd11581_e0\",\n",
    "        \"EX_cpd01017_e0\",\n",
    "        \"EX_cpd11590_e0\",\n",
    "        \"EX_cpd11592_e0\",\n",
    "        \"EX_cpd11584_e0\",\n",
    "        \"EX_cpd00731_e0\",\n",
    "        \"EX_cpd15603_e0\",\n",
    "        \"EX_cpd11587_e0\",\n",
    "        \"EX_cpd11582_e0\",\n",
    "        \"EX_cpd15606_e0\",\n",
    "        \"EX_cpd03424_e0\",\n",
    "        \"EX_cpd00423_e0\",\n",
    "        \"EX_cpd00080_e0\",\n",
    "        \"EX_cpd02233_e0\",\n",
    "        \"EX_cpd00355_e0\",\n",
    "        \"EX_cpd00235_e0\",\n",
    "        \"EX_cpd00079_e0\",\n",
    "        \"EX_cpd01912_e0\"\n",
    "    ]\n",
    "    reaction_probabilities = {}\n",
    "    for rxn in base_model.reactions:\n",
    "        highest_prob = None\n",
    "        for gene in rxn.genes:\n",
    "            if gene.id in feature_probabilities:\n",
    "                if highest_prob == None or feature_probabilities[gene.id] > highest_prob:\n",
    "                    highest_prob = feature_probabilities[gene.id]\n",
    "        if highest_prob != None:\n",
    "            rxn.probability = highest_prob\n",
    "            reaction_probabilities[rxn.id] = highest_prob\n",
    "        elif rxn.id[0:3] != \"bio\" and rxn.id[0:3] != \"EX_\" and rxn.id[0:3] != \"DM_\" and len(rxn.genes) == 0:\n",
    "            reaction_probabilities[rxn.id] = 0\n",
    "        else:\n",
    "            reaction_probabilities[rxn.id] = 0.05\n",
    "    min_prob = 0.05\n",
    "    prob_exp = 1\n",
    "    ex_weight = 1\n",
    "    kinetics_coef = 750\n",
    "    mdlutl = current_comm_model.mdlutl\n",
    "    pkgmgr = MSPackageManager.get_pkg_mgr(mdlutl)\n",
    "    #Setting media\n",
    "    pkgmgr.getpkg(\"KBaseMediaPkg\").build_package(complete_media)\n",
    "    #Adding commkinetic constraints\n",
    "    pkgmgr.getpkg(\"CommKineticPkg\").build_package(kinetics_coef, current_comm_model)\n",
    "    #Adding elemental uptake constraints\n",
    "    pkgmgr.getpkg(\"ElementUptakePkg\").build_package({\"C\": 300})\n",
    "    for rxn in base_model.reactions:\n",
    "        if \"EX_\" == rxn.id[0:3]:\n",
    "            currrxn = current_comm_model.model.reactions.get_by_id(rxn.id)\n",
    "            if rxn.id in dipeptide_exchanges:\n",
    "                currrxn.lower_bound = 0\n",
    "                currrxn.upper_bound = 0\n",
    "            else:\n",
    "                if currrxn.lower_bound < 0:\n",
    "                    currrxn.lower_bound = -1000\n",
    "                if currrxn.upper_bound > 0:\n",
    "                    currrxn.upper_bound = 1000\n",
    "    currrxn = current_comm_model.model.reactions.get_by_id(\"EX_cpd00007_e0\")\n",
    "    currrxn.lower_bound = -20\n",
    "    #Maximize biomass production\n",
    "    mdlutl.model.objective = mdlutl.model.problem.Objective(Zero, direction=\"max\")\n",
    "    mdlutl.model.objective.set_linear_coefficients({mdlutl.model.reactions.bio1.forward_variable: 1})\n",
    "    first_solution = mdlutl.model.optimize()\n",
    "    output[modelid][\"max_growth\"] = first_solution.objective_value\n",
    "    output[modelid][\"carbon_uptake\"] = pkgmgr.getpkg(\"ElementUptakePkg\").variables[\"elements\"][\"C\"].primal\n",
    "    records[0][modelid] = output[modelid][\"max_growth\"]\n",
    "    records[1][modelid] = output[modelid][\"carbon_uptake\"]\n",
    "    print(modelid+\" growth:\", output[modelid][\"max_growth\"])\n",
    "    print(modelid+\"carbon uptake:\",output[modelid][\"carbon_uptake\"])\n",
    "    \n",
    "    with open(util.output_dir+\"/\"+modelid+\"-growth.lp\", \"w\") as out:\n",
    "        out.write(str(mdlutl.model.solver))\n",
    "\n",
    "    if str(output[modelid][\"max_growth\"]) == \"nan\":\n",
    "        print(\"Skipping condition due to infeasibility\", modelid)\n",
    "        continue\n",
    "    #Constraining to 70% of community biomass\n",
    "    pkgmgr.getpkg(\"ObjConstPkg\").build_package(output[modelid][\"max_growth\"] * 0.5, None)\n",
    "    #Creating minimal probability objective\n",
    "    coef = {}\n",
    "    flux_fitting_target = {}\n",
    "    for rxn in base_model.reactions:\n",
    "        if \"rxn\" == rxn.id[0:3]:\n",
    "            probability = reaction_probabilities[rxn.id]\n",
    "            currrxn = mdlutl.model.reactions.get_by_id(rxn.id)\n",
    "            coef.update(\n",
    "                {\n",
    "                    currrxn.forward_variable: max(\n",
    "                        min_prob, (1 - float(probability) ** prob_exp)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "            coef.update(\n",
    "                {\n",
    "                    currrxn.reverse_variable: max(\n",
    "                        min_prob, (1 - float(probability) ** prob_exp)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        elif \"EX_\" == rxn.id[0:3]:\n",
    "            currrxn = mdlutl.model.reactions.get_by_id(rxn.id)\n",
    "            if rxn.id[3:11] in metabolomics_data:\n",
    "                flux_fitting_target[rxn.id] = -1*output[modelid][\"max_growth\"]*metabolomics_data[rxn.id[3:11]][modelid]\n",
    "            coef.update({currrxn.forward_variable: ex_weight})\n",
    "            coef.update({currrxn.reverse_variable: ex_weight})\n",
    "\n",
    "    #Solving the LP\n",
    "    #pkgmgr.getpkg(\"FluxFittingPkg\").build_package({\n",
    "    #    \"target_flux\": flux_fitting_target,\n",
    "    #    \"totalflux\": 0,\n",
    "    #    \"set_objective\": 1,\n",
    "    #    \"default_rescaling\": 0.1,\n",
    "    #    \"rescale_vfit_by_flux\": True\n",
    "    #})\n",
    "    stuck_reactions = {}\n",
    "    for iteration in range(1,11,1):\n",
    "        print(\"Iteration\",iteration)\n",
    "        for rxn in flux_fitting_target:\n",
    "            if rxn in stuck_reactions:\n",
    "                continue\n",
    "            currrxn = mdlutl.model.reactions.get_by_id(rxn)\n",
    "            starting_point = first_solution.fluxes[rxn]\n",
    "            distance = abs(flux_fitting_target[rxn] - starting_point)\n",
    "            original_upper = currrxn.upper_bound\n",
    "            original_lower = currrxn.lower_bound\n",
    "            if starting_point > flux_fitting_target[rxn]:\n",
    "                if starting_point-iteration*distance/10 < currrxn.upper_bound:\n",
    "                    currrxn.lower_bound = starting_point-iteration*distance/10\n",
    "                    currrxn.upper_bound = starting_point-iteration*distance/10\n",
    "                else:\n",
    "                    currrxn.upper_bound = starting_point-iteration*distance/10\n",
    "                    currrxn.lower_bound = starting_point-iteration*distance/10\n",
    "            else:\n",
    "                if flux_fitting_target[rxn]-(10-iteration)*distance/10 < currrxn.upper_bound:\n",
    "                    currrxn.lower_bound = flux_fitting_target[rxn]-(10-iteration)*distance/10\n",
    "                    currrxn.upper_bound = flux_fitting_target[rxn]-(10-iteration)*distance/10\n",
    "                else:\n",
    "                    currrxn.upper_bound = flux_fitting_target[rxn]-(10-iteration)*distance/10\n",
    "                    currrxn.lower_bound = flux_fitting_target[rxn]-(10-iteration)*distance/10\n",
    "            solution = mdlutl.model.optimize()\n",
    "            if solution.status != \"optimal\":\n",
    "                print(\"Stuck\",rxn,starting_point,flux_fitting_target[rxn],original_upper)\n",
    "                stuck_reactions[rxn] = 1\n",
    "                if original_upper >= currrxn.lower_bound:\n",
    "                    currrxn.upper_bound = original_upper\n",
    "                    currrxn.lower_bound = original_lower\n",
    "                else:\n",
    "                    currrxn.lower_bound = original_lower\n",
    "                    currrxn.upper_bound = original_upper\n",
    "    for rxn in flux_fitting_target:\n",
    "        currrxn = mdlutl.model.reactions.get_by_id(rxn)\n",
    "        print(currrxn.id,currrxn.lower_bound,currrxn.upper_bound)\n",
    "    #solution = mdlutl.model.optimize()\n",
    "    #output[modelid][\"flux fitting objective\"] = solution.objective_value\n",
    "    \n",
    "    #with open(util.output_dir+\"/\"+modelid+\"-fitting.lp\", \"w\") as out:\n",
    "    #    out.write(str(mdlutl.model.solver))\n",
    "\n",
    "    #records[2][modelid] = output[modelid][\"flux fitting objective\"]\n",
    "    #for rxn in flux_fitting_target:\n",
    "    #    currrxn = current_comm_model.model.reactions.get_by_id(rxn)\n",
    "    #    if solution.fluxes[rxn] > 0:\n",
    "    #        currrxn.upper_bound = solution.fluxes[rxn]\n",
    "    #        currrxn.lower_bound = solution.fluxes[rxn]\n",
    "    #    elif solution.fluxes[rxn] < 0:\n",
    "    #        currrxn.lower_bound = solution.fluxes[rxn]\n",
    "    #        currrxn.upper_bound = solution.fluxes[rxn]\n",
    "    #Setting probability minimization objective\n",
    "    mdlutl.model.objective = mdlutl.model.problem.Objective(Zero, direction=\"min\")\n",
    "    mdlutl.model.objective.set_linear_coefficients(coef)\n",
    "    solution = mdlutl.model.optimize()\n",
    "    if solution.status != \"optimal\":\n",
    "        while solution.status != \"optimal\":\n",
    "            print(\"Infeasible: reducing objective constraint\",0.9*mdlutl.pkgmgr.getpkg(\"ObjConstPkg\").constraints[\"objc\"][\"1\"].lb)\n",
    "            mdlutl.pkgmgr.getpkg(\"ObjConstPkg\").constraints[\"objc\"][\"1\"].lb = 0.9*mdlutl.pkgmgr.getpkg(\"ObjConstPkg\").constraints[\"objc\"][\"1\"].lb\n",
    "            solution = mdlutl.model.optimize()\n",
    "    output[modelid][\"minimum probability objective\"] = solution.objective_value\n",
    "    \n",
    "    with open(util.output_dir+\"/\"+modelid+\"-final.lp\", \"w\") as out:\n",
    "        out.write(str(mdlutl.model.solver))\n",
    "\n",
    "    records[3][modelid] = output[modelid][\"minimum probability objective\"]\n",
    "    output[modelid][\"solution\"] = {}\n",
    "    for rxn in mdlutl.model.reactions:\n",
    "        if rxn.id not in rxn_record_hash:\n",
    "            rxn_record_hash[rxn.id] = {\n",
    "                \"id\":rxn.id,\n",
    "            }\n",
    "            for mdl in model_list:\n",
    "                rxn_record_hash[rxn.id][mdl] = None\n",
    "            records.append(rxn_record_hash[rxn.id])\n",
    "        rxn_record_hash[rxn.id][modelid] = solution.fluxes[rxn.id]\n",
    "        output[modelid][\"solution\"][rxn.id] = solution.fluxes[rxn.id]\n",
    "    util.save(\"CommunityFluxSolution2\", output)\n",
    "    df = DataFrame.from_records(records)\n",
    "    df.to_csv(util.output_dir+\"/CommunityFluxSolution2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_ModelSEED",
   "language": "python",
   "name": "python3_modelseed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
